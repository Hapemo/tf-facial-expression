{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacf006c",
   "metadata": {},
   "source": [
    "This is for testing models with 3 specific layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0276c06-f50d-4eb0-bbca-9fc9ac880189",
   "metadata": {},
   "source": [
    "## Import libraries and set up functions\n",
    "Starting directory is this \"..\\\\asset\" from this notebook code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1bba16-1e5e-45d3-9b8c-d6bb28bb1c91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from keras.backend import clear_session\n",
    "from keras.backend import get_session\n",
    "import time\n",
    "import math\n",
    "from datetime import timedelta\n",
    "# import mediapipe as mp\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "os.chdir(\"..\\\\..\\\\asset\") # change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1611703-5ebe-4c2e-8629-859cfd5fa19f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotionList = ('Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt')\n",
    "TRAINANNOTATIONPATH = \"train_set\\\\annotations\\\\\"\n",
    "TRAINIMAGEPATH = \"train_set\\\\images\\\\\"\n",
    "TRAINLANDMARKPATH = \"train_set\\\\landmarks\\\\\"\n",
    "TESTANNOTATIONPATH = \"val_set\\\\annotations\\\\\"\n",
    "TESTIMAGEPATH = \"val_set\\\\images\\\\\"\n",
    "TESTLANDMARKPATH = \"val_set\\\\landmarks\\\\\"\n",
    "\n",
    "def LoadAllImageNames(filePath, limit = 0, catLimit = []):\n",
    "  \"\"\"Load the images names and label in tuple format (label, image name)\n",
    "\n",
    "  Args:\n",
    "      filePath (str): directory of image folder\n",
    "      limit (int, optional): max number of image to load. Defaults to 0.\n",
    "      catLimit (list<int>, optional): array of image count limit for each class. Defaults to []*8.\n",
    "\n",
    "  Returns:\n",
    "      list<str>: shuffled list of image names\n",
    "  \"\"\"\n",
    "  limitCounter = [0,0,0,0,0,0,0,0]\n",
    "  dataSet = []\n",
    "  loadCounter = 0\n",
    "\n",
    "  annotationPath = TRAINANNOTATIONPATH if (\"train\" in filePath) else TESTANNOTATIONPATH\n",
    "  for file in os.scandir(filePath):\n",
    "    if (limit > 0 and loadCounter > limit):\n",
    "      break\n",
    "  \n",
    "    name = file.name[:-4] # file name w/o file extension\n",
    "    if \"_landmark\" in name:\n",
    "      name = name[:-9]\n",
    "    data = np.load(\"{}{}_exp.npy\".format(annotationPath, name)) # \n",
    "    label = int(data.item(0))\n",
    "\n",
    "    if len(catLimit) > 0 and limitCounter[label] >= catLimit[label]:\n",
    "        continue\n",
    "    limitCounter[label] += 1\n",
    "    loadCounter += 1\n",
    "\n",
    "    dataSet.append((label, file.name))\n",
    "    if (loadCounter%10000==0):\n",
    "      print(\"Files loaded:{}\".format(loadCounter))\n",
    "  \n",
    "  print(\"Total images loaded: \", loadCounter)\n",
    "  print(\"Images Loaded: \", limitCounter)\n",
    "  random.shuffle(dataSet)\n",
    "  return dataSet\n",
    "\n",
    "# Load the pixels of a picture to numpy.ndarray format. false for test set, true for training set\n",
    "# Return image in RGB format\n",
    "def LoadImage(imagePath, imageName, normalize = True):\n",
    "  \"\"\"Load image using numpy\n",
    "\n",
    "  Args:\n",
    "      imagePath (str): image path \n",
    "      imageName (str): image name\n",
    "      normalize (bool, optional): To normalize image or not. Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "      numpy array: x,y,3 array\n",
    "  \"\"\"\n",
    "  # print(\"{}{}{}\".format(os.getcwd(), \"\\\\\"+imagePath, imageName))\n",
    "  image_array = cv2.imread(\"{}{}{}\".format(os.getcwd(), \"\\\\\"+imagePath, imageName))\n",
    "  image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
    "  if normalize:\n",
    "    image_array = image_array/255\n",
    "    \n",
    "  return image_array\n",
    "\n",
    "# Extract the daata from 0 to amount from list and return it\n",
    "def CropData(list, amount):\n",
    "  if (len(list) < amount):\n",
    "    amount = len(list)\n",
    "  croppedList = list[:amount]\n",
    "  del list[:amount]\n",
    "  return croppedList\n",
    "\n",
    "def LoadImages(imagePath, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entries in list:\n",
    "    try:\n",
    "      image = LoadImage(imagePath, entries[1])\n",
    "      data.append(image) \n",
    "      label.append(entries[0]) \n",
    "    except: \n",
    "      print(\"Failed to load training image: \", entries[1])\n",
    "  npLabel = np.array(label) \n",
    "  npData = np.array(data) \n",
    "  return npLabel, npData \n",
    "\n",
    "# Load a list of face mesh in landmark_pb2.NormalizedLandmarkList format\n",
    "def LoadFaceMeshes(faceMesh, path, list, withFace = False):\n",
    "  face = []\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    # try:\n",
    "    if withFace:\n",
    "      face.append(LoadImage(path, entry[1], normalize = False)) # Load image\n",
    "      data.append(faceMesh.process(face[-1])) # process it into face mesh and append into data\n",
    "    else:\n",
    "      data.append(faceMesh.process(LoadImage(path, entry[1], normalize = False))) # Load image, process it into face mesh, and append into data\n",
    "    label.append(entry[0])\n",
    "    # except:\n",
    "      # print(\"Failed to load training image: \", entry[1])\n",
    "\n",
    "  if withFace:\n",
    "    return np.array(label), np.array(data), np.array(face)\n",
    "  else:\n",
    "    return np.array(label), np.array(data)\n",
    "\n",
    "# Load face mesh data in numpy array of \n",
    "def LoadFaceMeshVertices(faceMesh, path, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    # load image, process it into normalizedlandmarklist format, obtain face data from the list, convert it into vertices\n",
    "    image = LoadImage(path, entry[1], normalize = False)\n",
    "    multiFaceLandmarks = faceMesh.process(image).multi_face_landmarks\n",
    "    if (multiFaceLandmarks == None):\n",
    "      print(\"{} multi-face landmarks is none\".format(entry[1]))\n",
    "      continue\n",
    "    vertices = MultiLandmarkToVertices(multiFaceLandmarks[0])\n",
    "    data.append(vertices)\n",
    "    label.append(entry[0])\n",
    "    # except:\n",
    "      # print(\"Failed to load training image: \", entry[1])\n",
    "\n",
    "def LoadVertices(path, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    vertices = np.load(\"{}{}{}.npy\".format(os.getcwd(), \"\\\\\"+path, entry[1][:-4]))\n",
    "    data.append(vertices)\n",
    "    label.append(entry[0])\n",
    "  \n",
    "  return np.array(label), np.array(data)\n",
    "  \n",
    "def MultiLandmarkToVertices(multiLandmark):\n",
    "  verticesList = []\n",
    "  for idx, landmark in enumerate(multiLandmark.landmark):\n",
    "    verticesList.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "  return verticesList\n",
    "\n",
    "\n",
    "def InitializeModel():\n",
    "  pretrained_model = tf.keras.applications.MobileNetV3Large(input_shape=(224,224,3)) # Initializing model with mobile net V3 pretrained model\n",
    "\n",
    "  # Initializing the input and output from the model, removing last layer\n",
    "  base_input = pretrained_model.layers[0].input\n",
    "  base_output = pretrained_model.layers[-2].output\n",
    "\n",
    "  # Adding 3 more layers to output side\n",
    "  final_output = layers.Dense(128)(base_output) # Adding new layers, to the output side\n",
    "  final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "  final_output = layers.Dense(64)(final_output)\n",
    "  final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "  final_output = layers.Dense(8, activation = 'softmax')(final_output) # 8 cuz there are 8 image classifications\n",
    "\n",
    "  new_model = keras.Model(inputs = base_input, outputs = final_output)\n",
    "  # new_model.summary()\n",
    "  return new_model\n",
    "\n",
    "def ConvertToGray(image):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "  return image\n",
    "\n",
    "def ScaleImage(image, width):\n",
    "  ratio = image.shape[1]/width\n",
    "  image = cv2.resize(image, (width, int(image.shape[0]/ratio)))\n",
    "  return image\n",
    "\n",
    "def DetectFace(image):\n",
    "  face_roi = np.ndarray(1)\n",
    "  faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "  grayImage = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  faces = faceCascade.detectMultiScale(grayImage, 1.3, 5)\n",
    "  for x,y,w,h in faces:\n",
    "    roi_gray = grayImage[y:y+h, x:x+w]\n",
    "    roi_color = image[y:y+h, x:x+w]\n",
    "    cv2.rectangle(image, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "    facess = faceCascade.detectMultiScale(roi_gray)\n",
    "    if (len(facess) == 0):\n",
    "      print(\"Face not detected\")\n",
    "    else:\n",
    "      for (ex,ey,ew,eh) in facess:\n",
    "        face_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "  return face_roi\n",
    "\n",
    "def ConvertToInput(image):\n",
    "  input = ScaleImage(image, 224)\n",
    "  input = np.expand_dims(input, axis = 0) ## to add fourth dimension to fit model input\n",
    "  input = input/255\n",
    "  return input\n",
    "\n",
    "def GetResult(model, input):\n",
    "  Predictions = model.predict(input)\n",
    "  print(Predictions)\n",
    "  result = np.argmax(Predictions)\n",
    "  return emotionList[result]\n",
    "\n",
    "def printDataSetLabels(dataSet):\n",
    "    counterList = list(range(8))\n",
    "    for label, name in dataSet:\n",
    "        counterList[label] += 1\n",
    "    print(counterList)\n",
    "    \n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    # try:\n",
    "    #     del classifier # this is from global space - change this as you need\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.9, visible_device_list=\"0\")\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.times = []\n",
    "\n",
    "  def on_epoch_begin(self, batch, logs={}):\n",
    "    self.epoch_time_start = time.time()\n",
    "\n",
    "  def on_epoch_end(self, batch, logs={}):\n",
    "    self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "  def AverageTime(self):\n",
    "    sum = 0\n",
    "    for time in self.times:\n",
    "      sum += time\n",
    "    return sum/len(self.times)\n",
    "\n",
    "def TestModel(model, testSetData):\n",
    "  # test model\n",
    "  lostSum = 0\n",
    "  accuracySum = 0\n",
    "  count = 0\n",
    "  while(len(testSetData) != 0):\n",
    "    # training data\n",
    "    try:\n",
    "      croppedList = CropData(testSetData, 100)\n",
    "      label, data = LoadImages(TESTIMAGEPATH, croppedList)\n",
    "      result = model.evaluate(data, label, batch_size = 1)\n",
    "      lostSum += result[0]\n",
    "      accuracySum += result[1]\n",
    "    except:\n",
    "      print(\"Failed to train data\")\n",
    "\n",
    "    count += 1\n",
    "    reset_keras()\n",
    "  \n",
    "  return lostSum/count, accuracySum/count\n",
    "\n",
    "def TestModelWithVertices(model, dataSet, tensorboard):\n",
    "  # test model\n",
    "  lostSum = 0\n",
    "  accuracySum = 0\n",
    "  count = 0\n",
    "  while(len(dataSet) != 0):\n",
    "    # training data\n",
    "    try:\n",
    "      croppedList = CropData(dataSet, 10000)\n",
    "      label, data = LoadVertices(TESTLANDMARKPATH, croppedList)\n",
    "      result = model.evaluate(data, label, batch_size = 1, callbacks =[tensorboard])\n",
    "      lostSum += result[0]\n",
    "      accuracySum += result[1]\n",
    "    except:\n",
    "      print(\"Failed to train data\")\n",
    "\n",
    "    count += 1\n",
    "    reset_keras()\n",
    "  \n",
    "  return lostSum/count, accuracySum/count\n",
    "\n",
    "def DrawFaceLandmark(label, image, landmarks):\n",
    "  mp_drawing = mp.solutions.drawing_utils\n",
    "  mp_drawing_styles = mp.solutions.drawing_styles\n",
    "  mp_face_mesh = mp.solutions.face_mesh\n",
    "  print(f'Face landmarks of {label}:')\n",
    "  if not landmarks.multi_face_landmarks:\n",
    "    print(\"unagle to locate face landmark for {}\".format(label))\n",
    "  annotated_image = image.copy()\n",
    "  for face_landmarks in landmarks.multi_face_landmarks:\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_iris_connections_style())\n",
    "  plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a2898-2d42-418a-9cef-a6cfb6324bfc",
   "metadata": {},
   "source": [
    "## Setup directory and import image file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b1a99c-a616-45b2-b662-fafad60524af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded:10000\n",
      "Files loaded:20000\n",
      "Files loaded:30000\n",
      "Total images loaded:  37553\n",
      "Images Loaded:  [5000, 5000, 5000, 5000, 5000, 3803, 5000, 3750]\n",
      "Total images loaded:  3987\n",
      "Images Loaded:  [499, 500, 498, 498, 498, 500, 499, 495]\n"
     ]
    }
   ],
   "source": [
    "# Get currect directory (os.getcwd() -> C:\\Users\\jazzt\\src)\n",
    "\n",
    "#-----------------------Start of code---------------------------\n",
    "# initialise image names and label\n",
    "mainTrainSet = LoadAllImageNames(TRAINLANDMARKPATH, catLimit=[5000]*8)\n",
    "mainTestSet = LoadAllImageNames(TESTLANDMARKPATH, catLimit=[500]*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcfdeee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reset learning rate callback to counter ReduceLROnPlateau\n",
    "class ResetLR(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, learningRate):\n",
    "        self.default_lr = learningRate\n",
    "    def on_train_begin(self, logs={}):\n",
    "        previous_lr = self.model.optimizer.lr.read_value()\n",
    "        if previous_lr != self.default_lr:\n",
    "            print(\"Resetting learning rate from {} to {}\".format(previous_lr, self.default_lr))\n",
    "            self.model.optimizer.lr.assign(self.default_lr)\n",
    "\n",
    "# # making stopper to only start counting from 3 epochs onwards\n",
    "# class CustomStopper(tf.keras.callbacks.EarlyStopping):\n",
    "#     def __init__(self, monitor='val_loss',\n",
    "#              min_delta=0, patience=3, verbose=0, mode='auto', start_epoch = 3): # add argument for starting epoch\n",
    "#         super(CustomStopper, self).__init__()\n",
    "#         self.start_epoch = start_epoch\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         if epoch > self.start_epoch:\n",
    "#             super().on_epoch_end(epoch, logs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847c16f0-a415-4767-9891-79849212a36c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\j.teoh\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image\n",
      "Epoch 1/30\n",
      "198/198 [==============================] - 14s 65ms/step - loss: 2.0666 - accuracy: 0.1534 - val_loss: 2.0589 - val_accuracy: 0.1665\n",
      "Epoch 2/30\n",
      "198/198 [==============================] - 12s 59ms/step - loss: 2.0267 - accuracy: 0.1867 - val_loss: 2.0164 - val_accuracy: 0.1925\n",
      "Epoch 3/30\n",
      "198/198 [==============================] - 12s 63ms/step - loss: 1.9883 - accuracy: 0.2145 - val_loss: 2.0035 - val_accuracy: 0.2000\n",
      "Epoch 4/30\n",
      "198/198 [==============================] - 13s 64ms/step - loss: 1.9773 - accuracy: 0.2201 - val_loss: 1.9852 - val_accuracy: 0.2145\n",
      "Epoch 5/30\n",
      "198/198 [==============================] - 13s 66ms/step - loss: 1.9602 - accuracy: 0.2304 - val_loss: 1.9727 - val_accuracy: 0.2242\n",
      "Epoch 6/30\n",
      "198/198 [==============================] - 13s 66ms/step - loss: 1.9582 - accuracy: 0.2314 - val_loss: 1.9358 - val_accuracy: 0.2546\n",
      "Epoch 7/30\n",
      "198/198 [==============================] - 13s 66ms/step - loss: 1.9449 - accuracy: 0.2425 - val_loss: 1.9253 - val_accuracy: 0.2619\n",
      "Epoch 8/30\n",
      "198/198 [==============================] - 13s 66ms/step - loss: 1.9394 - accuracy: 0.2432 - val_loss: 1.9291 - val_accuracy: 0.2595\n",
      "Epoch 9/30\n",
      "198/198 [==============================] - 12s 61ms/step - loss: 1.9311 - accuracy: 0.2487 - val_loss: 1.9331 - val_accuracy: 0.2491\n",
      "Epoch 10/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.9333 - accuracy: 0.2466 - val_loss: 2.0039 - val_accuracy: 0.2214\n",
      "Epoch 11/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.9218 - accuracy: 0.2563 - val_loss: 1.8918 - val_accuracy: 0.2746\n",
      "Epoch 12/30\n",
      "198/198 [==============================] - 12s 59ms/step - loss: 1.9116 - accuracy: 0.2632 - val_loss: 1.9898 - val_accuracy: 0.2325\n",
      "Epoch 13/30\n",
      "198/198 [==============================] - 11s 55ms/step - loss: 1.9082 - accuracy: 0.2612 - val_loss: 1.8927 - val_accuracy: 0.2808\n",
      "Epoch 14/30\n",
      "198/198 [==============================] - 11s 58ms/step - loss: 1.9037 - accuracy: 0.2656 - val_loss: 1.8808 - val_accuracy: 0.2817\n",
      "Epoch 15/30\n",
      "198/198 [==============================] - 11s 55ms/step - loss: 1.9044 - accuracy: 0.2661 - val_loss: 1.8934 - val_accuracy: 0.2762\n",
      "Epoch 16/30\n",
      "198/198 [==============================] - 11s 57ms/step - loss: 1.8995 - accuracy: 0.2670 - val_loss: 1.8816 - val_accuracy: 0.2820\n",
      "Epoch 17/30\n",
      "198/198 [==============================] - 11s 57ms/step - loss: 1.8909 - accuracy: 0.2751 - val_loss: 1.9017 - val_accuracy: 0.2794\n",
      "Epoch 18/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.8929 - accuracy: 0.2711 - val_loss: 1.8768 - val_accuracy: 0.2882\n",
      "Epoch 19/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.8911 - accuracy: 0.2726 - val_loss: 1.8823 - val_accuracy: 0.2822\n",
      "Epoch 20/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.8864 - accuracy: 0.2816 - val_loss: 1.9021 - val_accuracy: 0.2689\n",
      "Epoch 21/30\n",
      "198/198 [==============================] - 13s 65ms/step - loss: 1.8844 - accuracy: 0.2766 - val_loss: 1.8750 - val_accuracy: 0.2887\n",
      "Epoch 22/30\n",
      "198/198 [==============================] - 12s 63ms/step - loss: 1.8782 - accuracy: 0.2813 - val_loss: 1.8766 - val_accuracy: 0.2806\n",
      "Epoch 23/30\n",
      "198/198 [==============================] - 14s 69ms/step - loss: 1.8792 - accuracy: 0.2809 - val_loss: 1.8539 - val_accuracy: 0.2995\n",
      "Epoch 24/30\n",
      "198/198 [==============================] - 12s 62ms/step - loss: 1.8734 - accuracy: 0.2855 - val_loss: 1.8848 - val_accuracy: 0.2846\n",
      "Epoch 25/30\n",
      "198/198 [==============================] - 13s 65ms/step - loss: 1.8578 - accuracy: 0.2956 - val_loss: 1.8412 - val_accuracy: 0.3077\n",
      "Epoch 26/30\n",
      "198/198 [==============================] - 12s 61ms/step - loss: 1.8597 - accuracy: 0.2921 - val_loss: 1.8427 - val_accuracy: 0.3129\n",
      "Epoch 27/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.8595 - accuracy: 0.2921 - val_loss: 1.8545 - val_accuracy: 0.3026\n",
      "Epoch 28/30\n",
      "198/198 [==============================] - 13s 65ms/step - loss: 1.8563 - accuracy: 0.2924 - val_loss: 1.8551 - val_accuracy: 0.2958\n",
      "Epoch 29/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.8553 - accuracy: 0.2944 - val_loss: 1.8722 - val_accuracy: 0.2838\n",
      "12.228447963451517\n",
      "trained image count:  31553\n",
      "Images Left:  0\n",
      "Estimated completion time:  0:00:00\n",
      "---------------- flatten_30epoch_0.001lr_128bs_512_64_64_landmark Training Completed ------------------\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 1.8637 - accuracy: 0.2600\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 1.8649 - accuracy: 0.2800\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.9293 - accuracy: 0.2700\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 1.7502 - accuracy: 0.4000\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 1.9360 - accuracy: 0.3000\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 1.8552 - accuracy: 0.3100\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 1.9036 - accuracy: 0.2400\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 1.9633 - accuracy: 0.2600\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9589 - accuracy: 0.3100\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 1.8414 - accuracy: 0.3500\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 1.8410 - accuracy: 0.3200\n",
      "100/100 [==============================] - 2s 25ms/step - loss: 1.7574 - accuracy: 0.3800\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.8648 - accuracy: 0.2800\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 1.8843 - accuracy: 0.2300\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.9696 - accuracy: 0.2100\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.8344 - accuracy: 0.3300\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8777 - accuracy: 0.2400\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 1.8415 - accuracy: 0.3200\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.8882 - accuracy: 0.2700\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 1.8775 - accuracy: 0.2700\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 1.8381 - accuracy: 0.3100\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9339 - accuracy: 0.2600\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 1.8567 - accuracy: 0.2500\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.9029 - accuracy: 0.3200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.8218 - accuracy: 0.3700\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 1.9706 - accuracy: 0.2500\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 1.9088 - accuracy: 0.3200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8690 - accuracy: 0.2600\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8782 - accuracy: 0.2800\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.8040 - accuracy: 0.3600\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.9335 - accuracy: 0.2900\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.7681 - accuracy: 0.3300\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 1.8620 - accuracy: 0.3000\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.8242 - accuracy: 0.2600\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 1.9119 - accuracy: 0.2300\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 1.8909 - accuracy: 0.2700\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 1.7710 - accuracy: 0.3200\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 1.9312 - accuracy: 0.3100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 1.9061 - accuracy: 0.2200\n",
      "87/87 [==============================] - 2s 25ms/step - loss: 1.8557 - accuracy: 0.2874\n",
      "==========FINISH TESTING===========\n",
      "\n",
      "  model name: flatten_30epoch_0.001lr_128bs_512_64_64_landmark\n",
      "  average lost: 1.8735346853733064\n",
      "  average accuracy: 0.2906839083880186\n",
      "            \n",
      "loading image\n",
      "Epoch 1/30\n",
      "198/198 [==============================] - 13s 67ms/step - loss: 2.0693 - accuracy: 0.1532 - val_loss: 2.0667 - val_accuracy: 0.1551\n",
      "Epoch 2/30\n",
      "198/198 [==============================] - 13s 66ms/step - loss: 2.0293 - accuracy: 0.1800 - val_loss: 2.0300 - val_accuracy: 0.1841\n",
      "Epoch 3/30\n",
      "198/198 [==============================] - 13s 65ms/step - loss: 1.9918 - accuracy: 0.2087 - val_loss: 1.9979 - val_accuracy: 0.2068\n",
      "Epoch 4/30\n",
      "198/198 [==============================] - 12s 61ms/step - loss: 1.9689 - accuracy: 0.2245 - val_loss: 1.9842 - val_accuracy: 0.2079\n",
      "Epoch 5/30\n",
      "198/198 [==============================] - 13s 65ms/step - loss: 1.9588 - accuracy: 0.2324 - val_loss: 1.9785 - val_accuracy: 0.2198\n",
      "Epoch 6/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.9501 - accuracy: 0.2358 - val_loss: 1.9323 - val_accuracy: 0.2464\n",
      "Epoch 7/30\n",
      "198/198 [==============================] - 12s 61ms/step - loss: 1.9397 - accuracy: 0.2431 - val_loss: 1.9201 - val_accuracy: 0.2565\n",
      "Epoch 8/30\n",
      "198/198 [==============================] - 13s 66ms/step - loss: 1.9293 - accuracy: 0.2528 - val_loss: 1.9195 - val_accuracy: 0.2588\n",
      "Epoch 9/30\n",
      "198/198 [==============================] - 14s 68ms/step - loss: 1.9309 - accuracy: 0.2511 - val_loss: 1.9975 - val_accuracy: 0.2264\n",
      "Epoch 10/30\n",
      "198/198 [==============================] - 14s 69ms/step - loss: 1.9160 - accuracy: 0.2598 - val_loss: 1.8899 - val_accuracy: 0.2716\n",
      "Epoch 11/30\n",
      "198/198 [==============================] - 13s 67ms/step - loss: 1.9166 - accuracy: 0.2585 - val_loss: 1.8937 - val_accuracy: 0.2775\n",
      "Epoch 12/30\n",
      "198/198 [==============================] - 13s 67ms/step - loss: 1.9054 - accuracy: 0.2652 - val_loss: 1.9233 - val_accuracy: 0.2492\n",
      "Epoch 13/30\n",
      "198/198 [==============================] - 14s 68ms/step - loss: 1.9043 - accuracy: 0.2607 - val_loss: 1.9004 - val_accuracy: 0.2638\n",
      "Epoch 14/30\n",
      "198/198 [==============================] - 12s 63ms/step - loss: 1.9031 - accuracy: 0.2663 - val_loss: 1.9306 - val_accuracy: 0.2538\n",
      "12.895378913198199\n",
      "trained image count:  31553\n",
      "Images Left:  0\n",
      "Estimated completion time:  0:00:00\n",
      "---------------- flatten_30epoch_0.001lr_128bs_512_192_128_landmark Training Completed ------------------\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 1.9591 - accuracy: 0.2200\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.9129 - accuracy: 0.2900\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 1.9316 - accuracy: 0.2800\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.8845 - accuracy: 0.2600\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 1.9865 - accuracy: 0.2300\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 1.8849 - accuracy: 0.2800\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.9484 - accuracy: 0.2200\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 2.0256 - accuracy: 0.2200\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.9322 - accuracy: 0.3100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.8885 - accuracy: 0.2500\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 1.8669 - accuracy: 0.2500\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8612 - accuracy: 0.2700\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.8956 - accuracy: 0.1900\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8921 - accuracy: 0.2700\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 2.0138 - accuracy: 0.1900\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 1.8634 - accuracy: 0.2800\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 1.9154 - accuracy: 0.3100\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 1.9067 - accuracy: 0.2300\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9352 - accuracy: 0.3000\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9163 - accuracy: 0.2200\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 1.8799 - accuracy: 0.2500\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.9799 - accuracy: 0.2000\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 1.8915 - accuracy: 0.2800\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9181 - accuracy: 0.3200\n",
      "100/100 [==============================] - 2s 25ms/step - loss: 1.8787 - accuracy: 0.3100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 2.0191 - accuracy: 0.1800\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 1.9235 - accuracy: 0.3000\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 1.9241 - accuracy: 0.2500\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.8923 - accuracy: 0.3200\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 1.8566 - accuracy: 0.3100\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9601 - accuracy: 0.2400\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 1.8504 - accuracy: 0.3000\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.8877 - accuracy: 0.3400\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 1.9044 - accuracy: 0.1900\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9204 - accuracy: 0.2300\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 1.8990 - accuracy: 0.3300\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 1.7917 - accuracy: 0.3500\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9424 - accuracy: 0.3200\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 1.9385 - accuracy: 0.2400\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.9012 - accuracy: 0.3103\n",
      "==========FINISH TESTING===========\n",
      "\n",
      "  model name: flatten_30epoch_0.001lr_128bs_512_192_128_landmark\n",
      "  average lost: 1.9145034402608871\n",
      "  average accuracy: 0.26600862145423887\n",
      "            \n",
      "loading image\n",
      "Epoch 1/30\n",
      "198/198 [==============================] - 14s 68ms/step - loss: 2.0678 - accuracy: 0.1514 - val_loss: 2.0488 - val_accuracy: 0.1629\n",
      "Epoch 2/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 2.0396 - accuracy: 0.1727 - val_loss: 2.0342 - val_accuracy: 0.1716\n",
      "Epoch 3/30\n",
      "198/198 [==============================] - 12s 61ms/step - loss: 1.9951 - accuracy: 0.2045 - val_loss: 1.9655 - val_accuracy: 0.2185\n",
      "Epoch 4/30\n",
      "198/198 [==============================] - 12s 63ms/step - loss: 1.9744 - accuracy: 0.2204 - val_loss: 1.9628 - val_accuracy: 0.2291\n",
      "Epoch 5/30\n",
      "198/198 [==============================] - 11s 58ms/step - loss: 1.9592 - accuracy: 0.2261 - val_loss: 1.9705 - val_accuracy: 0.2229\n",
      "Epoch 6/30\n",
      "198/198 [==============================] - 12s 61ms/step - loss: 1.9482 - accuracy: 0.2373 - val_loss: 1.9554 - val_accuracy: 0.2340\n",
      "Epoch 7/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.9504 - accuracy: 0.2358 - val_loss: 1.9290 - val_accuracy: 0.2453\n",
      "Epoch 8/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.9318 - accuracy: 0.2489 - val_loss: 1.9963 - val_accuracy: 0.2298\n",
      "Epoch 9/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.9342 - accuracy: 0.2462 - val_loss: 1.9269 - val_accuracy: 0.2496\n",
      "Epoch 10/30\n",
      "198/198 [==============================] - 11s 56ms/step - loss: 1.9221 - accuracy: 0.2549 - val_loss: 1.9057 - val_accuracy: 0.2648\n",
      "Epoch 11/30\n",
      "198/198 [==============================] - 12s 59ms/step - loss: 1.9212 - accuracy: 0.2538 - val_loss: 1.9280 - val_accuracy: 0.2472\n",
      "Epoch 12/30\n",
      "198/198 [==============================] - 11s 56ms/step - loss: 1.9136 - accuracy: 0.2610 - val_loss: 1.8925 - val_accuracy: 0.2727\n",
      "Epoch 13/30\n",
      "198/198 [==============================] - 12s 60ms/step - loss: 1.9042 - accuracy: 0.2656 - val_loss: 1.8809 - val_accuracy: 0.2865\n",
      "Epoch 14/30\n",
      "198/198 [==============================] - 13s 66ms/step - loss: 1.8956 - accuracy: 0.2703 - val_loss: 1.8835 - val_accuracy: 0.2860\n",
      "Epoch 15/30\n",
      "198/198 [==============================] - 14s 72ms/step - loss: 1.8947 - accuracy: 0.2711 - val_loss: 1.9566 - val_accuracy: 0.2407\n",
      "Epoch 16/30\n",
      "198/198 [==============================] - 13s 67ms/step - loss: 1.9011 - accuracy: 0.2689 - val_loss: 1.9237 - val_accuracy: 0.2453\n",
      "Epoch 17/30\n",
      "198/198 [==============================] - 13s 66ms/step - loss: 1.8865 - accuracy: 0.2768 - val_loss: 1.9567 - val_accuracy: 0.2258\n",
      "12.27167527815875\n",
      "trained image count:  31553\n",
      "Images Left:  0\n",
      "Estimated completion time:  0:00:00\n",
      "---------------- flatten_30epoch_0.001lr_128bs_512_384_256_landmark Training Completed ------------------\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 1.9404 - accuracy: 0.2300\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 1.9031 - accuracy: 0.2800\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 1.9322 - accuracy: 0.2900\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 1.8584 - accuracy: 0.2800\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 1.9765 - accuracy: 0.2600\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.8709 - accuracy: 0.2500\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 1.9241 - accuracy: 0.2300\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0284 - accuracy: 0.1900\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9282 - accuracy: 0.3200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8729 - accuracy: 0.2800\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8574 - accuracy: 0.3500\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8454 - accuracy: 0.2900\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8906 - accuracy: 0.2300\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.8795 - accuracy: 0.3200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0160 - accuracy: 0.1500\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 1.8476 - accuracy: 0.3500\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 1.8943 - accuracy: 0.3200\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8747 - accuracy: 0.2800\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.9220 - accuracy: 0.2800\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.9119 - accuracy: 0.2700\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.8743 - accuracy: 0.3000\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 1.9594 - accuracy: 0.2400\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 1.8884 - accuracy: 0.2300\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9104 - accuracy: 0.3500\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8612 - accuracy: 0.2900\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0041 - accuracy: 0.2100\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9257 - accuracy: 0.2900\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9285 - accuracy: 0.2400\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8909 - accuracy: 0.3200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.8279 - accuracy: 0.3300\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.9373 - accuracy: 0.2100\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8540 - accuracy: 0.2900\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.8753 - accuracy: 0.3200\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 1.8950 - accuracy: 0.2100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.8957 - accuracy: 0.2600\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 1.9048 - accuracy: 0.3400\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.7809 - accuracy: 0.3200\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 1.9279 - accuracy: 0.3400\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 1.9250 - accuracy: 0.3300\n",
      "87/87 [==============================] - 3s 30ms/step - loss: 1.8846 - accuracy: 0.2989\n",
      "==========FINISH TESTING===========\n",
      "\n",
      "  model name: flatten_30epoch_0.001lr_128bs_512_384_256_landmark\n",
      "  average lost: 1.9031438320875167\n",
      "  average accuracy: 0.2792212624102831\n",
      "            \n",
      "loading image\n",
      "Epoch 1/30\n",
      "198/198 [==============================] - 13s 63ms/step - loss: 2.0751 - accuracy: 0.1467 - val_loss: 2.0529 - val_accuracy: 0.1553\n",
      "Epoch 2/30\n",
      "198/198 [==============================] - 12s 62ms/step - loss: 2.0320 - accuracy: 0.1827 - val_loss: 2.0030 - val_accuracy: 0.1970\n",
      "Epoch 3/30\n",
      "198/198 [==============================] - 12s 61ms/step - loss: 1.9966 - accuracy: 0.2079 - val_loss: 2.0029 - val_accuracy: 0.1857\n",
      "Epoch 4/30\n",
      "198/198 [==============================] - 12s 61ms/step - loss: 1.9760 - accuracy: 0.2222 - val_loss: 2.0153 - val_accuracy: 0.1947\n",
      "Epoch 5/30\n",
      "198/198 [==============================] - 11s 53ms/step - loss: 1.9604 - accuracy: 0.2297 - val_loss: 1.9617 - val_accuracy: 0.2163\n",
      "Epoch 6/30\n",
      "  9/198 [>.............................] - ETA: 11s - loss: 1.9576 - accuracy: 0.2179"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m thirdLayerSize \u001b[38;5;241m=\u001b[39m layerSize[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     17\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflatten_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mepoch_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mlr_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mbs_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_landmark\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, learningRate, batchSize, firstLayerSize, secondLayerSize, thirdLayerSize)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mFreezeLayerTrainAndTest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearningRate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirstLayerSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecondLayerSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthirdLayerSize\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 46\u001b[0m, in \u001b[0;36mFreezeLayerTrainAndTest\u001b[1;34m(epoch, learningRate, modelName, batchSize, firstLayerSize, secondLayerSize, thirdLayerSize)\u001b[0m\n\u001b[0;32m     44\u001b[0m   label, data \u001b[38;5;241m=\u001b[39m LoadVertices(TRAINLANDMARKPATH, croppedList)\n\u001b[0;32m     45\u001b[0m   \u001b[38;5;66;03m# Training model\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m   \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtime_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlyStop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# earlyStop, earlyStopAcc, reduce_lr, reset_lr\u001b[39;00m\n\u001b[0;32m     47\u001b[0m   \u001b[38;5;28mprint\u001b[39m(time_callback\u001b[38;5;241m.\u001b[39mAverageTime())\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\engine\\training.py:1021\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_function\u001b[39m(iterator):\n\u001b[0;32m   1020\u001b[0m   \u001b[38;5;124;03m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1021\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstep_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\engine\\training.py:1009\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile:\n\u001b[0;32m   1007\u001b[0m   run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[0;32m   1008\u001b[0m       run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, experimental_relax_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1009\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1010\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(run_step, args\u001b[38;5;241m=\u001b[39m(data,))\n\u001b[0;32m   1011\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1012\u001b[0m     outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:836\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    835\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:819\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 819\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:2917\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   2916\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2917\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2918\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIteratorGetNext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2919\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   2921\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SaveInformation = [] # Saving information as {name, bs, L1, L2, L3, L4, loss, accuracy}\n",
    "\n",
    "batchSizes = [128, 256, 384]\n",
    "layerSizes = [\n",
    "  (512, 64, 64), (512, 192, 128), (512, 384, 256), \n",
    "  (764, 128, 48), (764, 128, 128), (764, 192, 128), (764, 192, 192), (764, 256, 256), \n",
    "  (1024, 256, 128), (1024, 384, 128)\n",
    "]\n",
    "epoch = 30\n",
    "learningRate = 0.001\n",
    "\n",
    "for batchSize in batchSizes:\n",
    "  for layerSize in layerSizes:\n",
    "    firstLayerSize = layerSize[0]\n",
    "    secondLayerSize = layerSize[1]\n",
    "    thirdLayerSize = layerSize[2]\n",
    "    name = \"flatten_{}epoch_{}lr_{}bs_{}_{}_{}_landmark\".format(epoch, learningRate, batchSize, firstLayerSize, secondLayerSize, thirdLayerSize)\n",
    "    FreezeLayerTrainAndTest(epoch, learningRate, name, batchSize, firstLayerSize, secondLayerSize, thirdLayerSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13305a89-e837-4804-a64a-81c8162168ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSizes = [128, 256, 384]\n",
    "firstLayerSizes = [512, 764, 1024, 1536, 2048]\n",
    "secondLayerSizes = [64, 128, 192, 256, 384, 512, 764, 1024]\n",
    "thirdLayerSizes = [64, 128, 192, 256, 384, 512, 764, 1024]\n",
    "fourthLayerSizes = [16, 32, 48, 64, 128, 192, 256]\n",
    "epoch = 30\n",
    "learningRate = 0.001\n",
    "\n",
    "for batchSize in batchSizes:\n",
    "  for firstLayerSize in firstLayerSizes:\n",
    "    for secondLayerSize in secondLayerSizes:\n",
    "      if secondLayerSize > firstLayerSize:\n",
    "        continue\n",
    "      for thirdLayerSize in thirdLayerSizes:\n",
    "        if (thirdLayerSize > secondLayerSize):\n",
    "          continue\n",
    "        for fourthLayerSize in fourthLayerSizes:\n",
    "          if (fourthLayerSize > thirdLayerSize):\n",
    "            continue\n",
    "          name = \"freeze_{}epoch_{}lr_{}bs_{}_{}_{}_{}_landmark\".format(epoch, learningRate, batchSize, firstLayerSize, secondLayerSize, thirdLayerSize, fourthLayerSize)\n",
    "          FreezeLayerTrainAndTest(epoch, learningRate, name, batchSize, firstLayerSize, secondLayerSize, thirdLayerSize, fourthLayerSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af04fac-a41d-4009-ac2b-3e838fe4c778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def FreezeLayerTrainAndTest(epoch, learningRate, modelName, batchSize, firstLayerSize, secondLayerSize, thirdLayerSize):\n",
    "  \"\"\"Train and test a model. This function's input should be customized to accept different parameters, function body should be customized to these parameters to train models of variety of paramters.\n",
    "\n",
    "  Args:\n",
    "      modelName (str): name of model\n",
    "  \"\"\"\n",
    "  #-------------------------------------------------\n",
    "  # Building model\n",
    "  #-------------------------------------------------\n",
    "  # Adding 3 more layers to output side\n",
    "  inputs = tf.keras.Input(shape=(478,3))\n",
    "  base = tf.keras.layers.Dense(firstLayerSize, activation='relu')(inputs)\n",
    "  base = tf.keras.layers.Dense(secondLayerSize, activation='relu')(base)\n",
    "  base = tf.keras.layers.Dense(thirdLayerSize, activation='relu')(base)\n",
    "  base = tf.keras.layers.Flatten()(base)\n",
    "  outputs = tf.keras.layers.Dense(8, activation='softmax')(base)\n",
    "  \n",
    "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = keras.optimizers.Adam(lr = learningRate), metrics=[\"accuracy\"])\n",
    "\n",
    "  # ------------------ Callbacks ------------------\n",
    "  earlyStop = EarlyStopping(monitor='val_loss', patience = 4, restore_best_weights = True)\n",
    "  # earlyStopAcc = EarlyStopping(monitor='val_accuracy', patience = 1, restore_best_weights = True)\n",
    "  # reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 2, min_lr = 0.0002)\n",
    "  # reset_lr = ResetLR(learningRate)\n",
    "\n",
    "  # ------------------ Training Model ------------------\n",
    "  time_callback = TimeHistory()\n",
    "  trainDataSet = mainTrainSet.copy()\n",
    "\n",
    "  # Initializing model fit params\n",
    "  # batchSize = 4\n",
    "  imgPerIter = batchSize*256\n",
    "\n",
    "  count = 0\n",
    "  iter = 0\n",
    "  while(len(trainDataSet) != 0):\n",
    "    iter += 1\n",
    "    tensorboard = TensorBoard(log_dir = \"logs/{}_it{}\".format(modelName, iter))\n",
    "    croppedList = []\n",
    "    try:\n",
    "      print(\"loading image\") # Crop and load images in\n",
    "      croppedList = CropData(trainDataSet, imgPerIter)\n",
    "      label, data = LoadVertices(TRAINLANDMARKPATH, croppedList)\n",
    "      # Training model\n",
    "      model.fit(data, label, epochs = epoch, validation_split = 0.2, batch_size = batchSize, callbacks = [time_callback, tensorboard, earlyStop]) # earlyStop, earlyStopAcc, reduce_lr, reset_lr\n",
    "      print(time_callback.AverageTime())\n",
    "    except RuntimeError as e:\n",
    "      print(e)\n",
    "    \n",
    "    # Saving weights\n",
    "    model.save_weights('{}.h5'.format(modelName))\n",
    "    \n",
    "    # Print summary of current iteration\n",
    "    count = count + len(croppedList)\n",
    "    timeLeft = len(trainDataSet)/imgPerIter * time_callback.AverageTime()\n",
    "    print(\"trained image count: \", count)\n",
    "    print(\"Images Left: \", len(trainDataSet))\n",
    "    print(\"Estimated completion time: \", str(timedelta(seconds=timeLeft)))\n",
    "    \n",
    "    reset_keras()\n",
    "  print(\"---------------- {} Training Completed ------------------\".format(modelName))\n",
    "\n",
    "  # ---------------- Testing Model -----------------\n",
    "  # initialise image names and label\n",
    "  testSetData = mainTestSet.copy()\n",
    "\n",
    "  # test model\n",
    "  loss, accuracy = TestModelWithVertices(model, testSetData, tensorboard)\n",
    "\n",
    "  # Saving test information on notebook\n",
    "  SaveInformation.append({\"name\": modelName, \n",
    "                          \"batch\": batchSize, \n",
    "                          \"L1\": firstLayerSize, \n",
    "                          \"L2\": secondLayerSize, \n",
    "                          \"L3\": thirdLayerSize, \n",
    "                          \"loss\": loss, \n",
    "                          \"accuracy\": accuracy })\n",
    "\n",
    "  # Saving test informations\n",
    "  modelSummary = '''\n",
    "  model name: {}\n",
    "  average lost: {}\n",
    "  average accuracy: {}\n",
    "            '''.format(modelName, loss, accuracy)\n",
    "      \n",
    "  print(\"==========FINISH TESTING===========\\n{}\".format(modelSummary))\n",
    "  \n",
    "  file = open(\"training_summaries.txt\", \"a\")\n",
    "  file.write(modelSummary + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f35c1-9369-4052-88c4-e38a5146dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b9aab-4d6b-4547-9f7b-fc718d857e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for info in SaveInformation:\n",
    "    print(info['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e18d46a7-4bbe-4efb-aa55-f133edab3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FreezeLayerTrainAndTesttesting(epoch, learningRate, modelName, batchSize, firstLayerSize, secondLayerSize, thirdLayerSize, fourthLayerSize):\n",
    "  \"\"\"Train and test a model. This function's input should be customized to accept different parameters, function body should be customized to these parameters to train models of variety of paramters.\n",
    "\n",
    "  Args:\n",
    "      modelName (str): name of model\n",
    "  \"\"\"\n",
    "  #-------------------------------------------------\n",
    "  # Building model\n",
    "  #-------------------------------------------------\n",
    "  # Adding 3 more layers to output side\n",
    "  inputs = tf.keras.Input(shape=(478,3))\n",
    "  base = tf.keras.layers.Dense(firstLayerSize, activation='relu')(inputs)\n",
    "  base = tf.keras.layers.Dense(secondLayerSize, activation='relu')(base)\n",
    "  base = tf.keras.layers.Flatten()(base)\n",
    "  base = tf.keras.layers.Dense(thirdLayerSize, activation='relu')(base)\n",
    "  base = tf.keras.layers.Dense(fourthLayerSize, activation='relu')(base)\n",
    "  outputs = tf.keras.layers.Dense(8, activation='softmax')(base)\n",
    "  \n",
    "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = keras.optimizers.Adam(lr = learningRate), metrics=[\"accuracy\"])\n",
    "\n",
    "  # ------------------ Callbacks ------------------\n",
    "  # earlyStop = EarlyStopping(monitor='val_loss', patience = 4, restore_best_weights = True)\n",
    "  # earlyStopAcc = EarlyStopping(monitor='val_accuracy', patience = 1, restore_best_weights = True)\n",
    "  # reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 2, min_lr = 0.0002)\n",
    "  # reset_lr = ResetLR(learningRate)\n",
    "\n",
    "  # ------------------ Training Model ------------------\n",
    "  time_callback = TimeHistory()\n",
    "  trainDataSet = mainTrainSet.copy()\n",
    "\n",
    "  # Initializing model fit params\n",
    "  # batchSize = 4\n",
    "  imgPerIter = batchSize*500\n",
    "\n",
    "  count = 0\n",
    "  iter = 0\n",
    "  while(len(trainDataSet) != 0):\n",
    "    iter += 1\n",
    "    tensorboard = TensorBoard(log_dir = \"logs/{}_it{}\".format(modelName, iter))\n",
    "    croppedList = []\n",
    "    try:\n",
    "      print(\"loading image\") # Crop and load images in\n",
    "      croppedList = CropData(trainDataSet, imgPerIter)\n",
    "      label, data = LoadVertices(TRAINLANDMARKPATH, croppedList)\n",
    "      # Training model\n",
    "      model.fit(data, label, epochs = epoch, validation_split = 0.2, batch_size = batchSize, callbacks = [time_callback, tensorboard]) # earlyStop, earlyStopAcc, reduce_lr, reset_lr\n",
    "      print(time_callback.AverageTime())\n",
    "    except RuntimeError as e:\n",
    "      print(e)\n",
    "    \n",
    "    # Saving weights\n",
    "    model.save_weights('{}.h5'.format(modelName))\n",
    "    \n",
    "    # Print summary of current iteration\n",
    "    count = count + len(croppedList)\n",
    "    timeLeft = len(trainDataSet)/imgPerIter * time_callback.AverageTime()\n",
    "    print(\"trained image count: \", count)\n",
    "    print(\"Images Left: \", len(trainDataSet))\n",
    "    print(\"Estimated completion time: \", str(timedelta(seconds=timeLeft)))\n",
    "    \n",
    "    reset_keras()\n",
    "  print(\"---------------- {} Training Completed ------------------\".format(modelName))\n",
    "\n",
    "  # ---------------- Testing Model -----------------\n",
    "  # initialise image names and label\n",
    "  testSetData = mainTestSet.copy()\n",
    "\n",
    "  # test model\n",
    "  loss, accuracy = TestModelWithVertices(model, testSetData, tensorboard)\n",
    "\n",
    "  # Saving test information on notebook\n",
    "  SaveInformation.append({\"name\": modelName, \n",
    "                          \"batch\": batchSize, \n",
    "                          \"L1\": firstLayerSize, \n",
    "                          \"L2\": secondLayerSize, \n",
    "                          \"L3\": thirdLayerSize, \n",
    "                          \"L3\": fourthLayerSize, \n",
    "                          \"loss\": loss, \n",
    "                          \"accuracy\": accuracy })\n",
    "\n",
    "  # Saving test informations\n",
    "  modelSummary = '''\n",
    "  model name: {}\n",
    "  average lost: {}\n",
    "  average accuracy: {}\n",
    "            '''.format(modelName, loss, accuracy)\n",
    "      \n",
    "  print(\"==========FINISH TESTING===========\\n{}\".format(modelSummary))\n",
    "  \n",
    "  file = open(\"training_summaries.txt\", \"a\")\n",
    "  file.write(modelSummary + \"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85898ff3-acef-461c-9b4b-fcd0cd160048",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jazzt\\anaconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image\n",
      "Epoch 1/40\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 2.0326 - accuracy: 0.1914 - val_loss: 1.9868 - val_accuracy: 0.2198\n",
      "Epoch 2/40\n",
      "235/235 [==============================] - 18s 76ms/step - loss: 1.9478 - accuracy: 0.2386 - val_loss: 1.9404 - val_accuracy: 0.2396\n",
      "Epoch 3/40\n",
      "235/235 [==============================] - 19s 81ms/step - loss: 1.9361 - accuracy: 0.2446 - val_loss: 1.9304 - val_accuracy: 0.2459\n",
      "Epoch 4/40\n",
      "235/235 [==============================] - 19s 81ms/step - loss: 1.9255 - accuracy: 0.2521 - val_loss: 1.9120 - val_accuracy: 0.2617\n",
      "Epoch 5/40\n",
      "235/235 [==============================] - 19s 80ms/step - loss: 1.9185 - accuracy: 0.2575 - val_loss: 1.9109 - val_accuracy: 0.2624\n",
      "Epoch 6/40\n",
      "235/235 [==============================] - 19s 81ms/step - loss: 1.9034 - accuracy: 0.2714 - val_loss: 1.9051 - val_accuracy: 0.2641\n",
      "Epoch 7/40\n",
      "235/235 [==============================] - 19s 79ms/step - loss: 1.8837 - accuracy: 0.2813 - val_loss: 1.8934 - val_accuracy: 0.2691\n",
      "Epoch 8/40\n",
      "235/235 [==============================] - 18s 79ms/step - loss: 1.8719 - accuracy: 0.2857 - val_loss: 1.8514 - val_accuracy: 0.2917\n",
      "Epoch 9/40\n",
      "133/235 [===============>..............] - ETA: 7s - loss: 1.8663 - accuracy: 0.2876"
     ]
    }
   ],
   "source": [
    "SaveInformation = []\n",
    "batchSizes = [128, 96]\n",
    "firstLayerSizes = [196]\n",
    "secondLayerSizes = [64]\n",
    "thirdLayerSizes = [512]\n",
    "fourthLayerSizes = [64]\n",
    "epoch = 40\n",
    "learningRate = 0.002\n",
    "for batchSize in batchSizes:\n",
    "  for firstLayerSize in firstLayerSizes:\n",
    "    for secondLayerSize in secondLayerSizes:\n",
    "      if secondLayerSize > firstLayerSize:\n",
    "        continue\n",
    "      for thirdLayerSize in thirdLayerSizes:\n",
    "        for fourthLayerSize in fourthLayerSizes:\n",
    "          if (fourthLayerSize > thirdLayerSize):\n",
    "            continue\n",
    "          name = \"ffdffFinal_{}epoch_{}lr_{}bs_{}_{}_{}_{}_landmark\".format(epoch, learningRate, batchSize, firstLayerSize, secondLayerSize, thirdLayerSize, fourthLayerSize)\n",
    "          FreezeLayerTrainAndTesttesting(epoch, learningRate, name, batchSize, firstLayerSize, secondLayerSize, thirdLayerSize, fourthLayerSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1085094-52c4-41d9-8eb2-3811d175a724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
