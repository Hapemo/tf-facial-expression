{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0276c06-f50d-4eb0-bbca-9fc9ac880189",
   "metadata": {},
   "source": [
    "## Import libraries and set up functions\n",
    "Starting directory is this \"..\\\\asset\" from this notebook code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1bba16-1e5e-45d3-9b8c-d6bb28bb1c91",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from keras.backend import clear_session\n",
    "from keras.backend import get_session\n",
    "import time\n",
    "import math\n",
    "from datetime import timedelta\n",
    "# import mediapipe as mp\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "os.chdir(\"..\\\\asset\") # change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1611703-5ebe-4c2e-8629-859cfd5fa19f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotionList = ('Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt')\n",
    "TRAINANNOTATIONPATH = \"train_set\\\\annotations\\\\\"\n",
    "TRAINIMAGEPATH = \"train_set\\\\images\\\\\"\n",
    "TRAINLANDMARKPATH = \"train_set\\\\landmarks\\\\\"\n",
    "TESTANNOTATIONPATH = \"val_set\\\\annotations\\\\\"\n",
    "TESTIMAGEPATH = \"val_set\\\\images\\\\\"\n",
    "TESTLANDMARKPATH = \"val_set\\\\landmarks\\\\\"\n",
    "\n",
    "def LoadAllImageNames(filePath, limit = 0, catLimit = []):\n",
    "  \"\"\"Load the images names and label in tuple format (label, image name)\n",
    "\n",
    "  Args:\n",
    "      filePath (str): directory of image folder\n",
    "      limit (int, optional): max number of image to load. Defaults to 0.\n",
    "      catLimit (list<int>, optional): array of image count limit for each class. Defaults to []*8.\n",
    "\n",
    "  Returns:\n",
    "      list<str>: shuffled list of image names\n",
    "  \"\"\"\n",
    "  limitCounter = [0,0,0,0,0,0,0,0]\n",
    "  dataSet = []\n",
    "  loadCounter = 0\n",
    "\n",
    "  annotationPath = TRAINANNOTATIONPATH if (\"train\" in filePath) else TESTANNOTATIONPATH\n",
    "  for file in os.scandir(filePath):\n",
    "    if (limit > 0 and loadCounter > limit):\n",
    "      break\n",
    "  \n",
    "    name = file.name[:-4] # file name w/o file extension\n",
    "    if \"_landmark\" in name:\n",
    "      name = name[:-9]\n",
    "    data = np.load(\"{}{}_exp.npy\".format(annotationPath, name)) # \n",
    "    label = int(data.item(0))\n",
    "\n",
    "    if len(catLimit) > 0 and limitCounter[label] >= catLimit[label]:\n",
    "        continue\n",
    "    limitCounter[label] += 1\n",
    "    loadCounter += 1\n",
    "\n",
    "    dataSet.append((label, file.name))\n",
    "    if (loadCounter%10000==0):\n",
    "      print(\"Files loaded:{}\".format(loadCounter))\n",
    "  \n",
    "  print(\"Total images loaded: \", loadCounter)\n",
    "  print(\"Images Loaded: \", limitCounter)\n",
    "  random.shuffle(dataSet)\n",
    "  return dataSet\n",
    "\n",
    "# Load the pixels of a picture to numpy.ndarray format. false for test set, true for training set\n",
    "# Return image in RGB format\n",
    "def LoadImage(imagePath, imageName, normalize = True):\n",
    "  \"\"\"Load image using numpy\n",
    "\n",
    "  Args:\n",
    "      imagePath (str): image path \n",
    "      imageName (str): image name\n",
    "      normalize (bool, optional): To normalize image or not. Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "      numpy array: x,y,3 array\n",
    "  \"\"\"\n",
    "  # print(\"{}{}{}\".format(os.getcwd(), \"\\\\\"+imagePath, imageName))\n",
    "  image_array = cv2.imread(\"{}{}{}\".format(os.getcwd(), \"\\\\\"+imagePath, imageName))\n",
    "  image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
    "  if normalize:\n",
    "    image_array = image_array/255\n",
    "    \n",
    "  return image_array\n",
    "\n",
    "# Extract the daata from 0 to amount from list and return it\n",
    "def CropData(list, amount):\n",
    "  if (len(list) < amount):\n",
    "    amount = len(list)\n",
    "  croppedList = list[:amount]\n",
    "  del list[:amount]\n",
    "  return croppedList\n",
    "\n",
    "def LoadImages(imagePath, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entries in list:\n",
    "    try:\n",
    "      image = LoadImage(imagePath, entries[1])\n",
    "      data.append(image) \n",
    "      label.append(entries[0]) \n",
    "    except: \n",
    "      print(\"Failed to load training image: \", entries[1])\n",
    "  npLabel = np.array(label) \n",
    "  npData = np.array(data) \n",
    "  return npLabel, npData \n",
    "\n",
    "# Load a list of face mesh in landmark_pb2.NormalizedLandmarkList format\n",
    "def LoadFaceMeshes(faceMesh, path, list, withFace = False):\n",
    "  face = []\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    # try:\n",
    "    if withFace:\n",
    "      face.append(LoadImage(path, entry[1], normalize = False)) # Load image\n",
    "      data.append(faceMesh.process(face[-1])) # process it into face mesh and append into data\n",
    "    else:\n",
    "      data.append(faceMesh.process(LoadImage(path, entry[1], normalize = False))) # Load image, process it into face mesh, and append into data\n",
    "    label.append(entry[0])\n",
    "    # except:\n",
    "      # print(\"Failed to load training image: \", entry[1])\n",
    "\n",
    "  if withFace:\n",
    "    return np.array(label), np.array(data), np.array(face)\n",
    "  else:\n",
    "    return np.array(label), np.array(data)\n",
    "\n",
    "# Load face mesh data in numpy array of \n",
    "def LoadFaceMeshVertices(faceMesh, path, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    # load image, process it into normalizedlandmarklist format, obtain face data from the list, convert it into vertices\n",
    "    image = LoadImage(path, entry[1], normalize = False)\n",
    "    multiFaceLandmarks = faceMesh.process(image).multi_face_landmarks\n",
    "    if (multiFaceLandmarks == None):\n",
    "      print(\"{} multi-face landmarks is none\".format(entry[1]))\n",
    "      continue\n",
    "    vertices = MultiLandmarkToVertices(multiFaceLandmarks[0])\n",
    "    data.append(vertices)\n",
    "    label.append(entry[0])\n",
    "    # except:\n",
    "      # print(\"Failed to load training image: \", entry[1])\n",
    "\n",
    "def LoadVertices(path, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    vertices = np.load(\"{}{}{}.npy\".format(os.getcwd(), \"\\\\\"+path, entry[1][:-4]))\n",
    "    data.append(vertices)\n",
    "    label.append(entry[0])\n",
    "  \n",
    "  return np.array(label), np.array(data)\n",
    "  \n",
    "def MultiLandmarkToVertices(multiLandmark):\n",
    "  verticesList = []\n",
    "  for idx, landmark in enumerate(multiLandmark.landmark):\n",
    "    verticesList.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "  return verticesList\n",
    "\n",
    "\n",
    "def InitializeModel():\n",
    "  pretrained_model = tf.keras.applications.MobileNetV3Large(input_shape=(224,224,3)) # Initializing model with mobile net V3 pretrained model\n",
    "\n",
    "  # Initializing the input and output from the model, removing last layer\n",
    "  base_input = pretrained_model.layers[0].input\n",
    "  base_output = pretrained_model.layers[-2].output\n",
    "\n",
    "  # Adding 3 more layers to output side\n",
    "  final_output = layers.Dense(128)(base_output) # Adding new layers, to the output side\n",
    "  final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "  final_output = layers.Dense(64)(final_output)\n",
    "  final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "  final_output = layers.Dense(8, activation = 'softmax')(final_output) # 8 cuz there are 8 image classifications\n",
    "\n",
    "  new_model = keras.Model(inputs = base_input, outputs = final_output)\n",
    "  # new_model.summary()\n",
    "  return new_model\n",
    "\n",
    "def ConvertToGray(image):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "  return image\n",
    "\n",
    "def ScaleImage(image, width):\n",
    "  ratio = image.shape[1]/width\n",
    "  image = cv2.resize(image, (width, int(image.shape[0]/ratio)))\n",
    "  return image\n",
    "\n",
    "def DetectFace(image):\n",
    "  face_roi = np.ndarray(1)\n",
    "  faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "  grayImage = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  faces = faceCascade.detectMultiScale(grayImage, 1.3, 5)\n",
    "  for x,y,w,h in faces:\n",
    "    roi_gray = grayImage[y:y+h, x:x+w]\n",
    "    roi_color = image[y:y+h, x:x+w]\n",
    "    cv2.rectangle(image, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "    facess = faceCascade.detectMultiScale(roi_gray)\n",
    "    if (len(facess) == 0):\n",
    "      print(\"Face not detected\")\n",
    "    else:\n",
    "      for (ex,ey,ew,eh) in facess:\n",
    "        face_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "  return face_roi\n",
    "\n",
    "def ConvertToInput(image):\n",
    "  input = ScaleImage(image, 224)\n",
    "  input = np.expand_dims(input, axis = 0) ## to add fourth dimension to fit model input\n",
    "  input = input/255\n",
    "  return input\n",
    "\n",
    "def GetResult(model, input):\n",
    "  Predictions = model.predict(input)\n",
    "  print(Predictions)\n",
    "  result = np.argmax(Predictions)\n",
    "  return emotionList[result]\n",
    "\n",
    "def printDataSetLabels(dataSet):\n",
    "    counterList = list(range(8))\n",
    "    for label, name in dataSet:\n",
    "        counterList[label] += 1\n",
    "    print(counterList)\n",
    "    \n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    # try:\n",
    "    #     del classifier # this is from global space - change this as you need\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.9, visible_device_list=\"0\")\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.times = []\n",
    "\n",
    "  def on_epoch_begin(self, batch, logs={}):\n",
    "    self.epoch_time_start = time.time()\n",
    "\n",
    "  def on_epoch_end(self, batch, logs={}):\n",
    "    self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "  def AverageTime(self):\n",
    "    sum = 0\n",
    "    for time in self.times:\n",
    "      sum += time\n",
    "    return sum/len(self.times)\n",
    "\n",
    "def TestModel(model, testSetData):\n",
    "  # test model\n",
    "  lostSum = 0\n",
    "  accuracySum = 0\n",
    "  count = 0\n",
    "  while(len(testSetData) != 0):\n",
    "    # training data\n",
    "    try:\n",
    "      croppedList = CropData(testSetData, 100)\n",
    "      label, data = LoadImages(TESTIMAGEPATH, croppedList)\n",
    "      result = model.evaluate(data, label, batch_size = 1)\n",
    "      lostSum += result[0]\n",
    "      accuracySum += result[1]\n",
    "    except:\n",
    "      print(\"Failed to train data\")\n",
    "\n",
    "    count += 1\n",
    "    reset_keras()\n",
    "  \n",
    "  return lostSum/count, accuracySum/count\n",
    "\n",
    "def TestModelWithVertices(model, dataSet, tensorboard):\n",
    "  # test model\n",
    "  lostSum = 0\n",
    "  accuracySum = 0\n",
    "  count = 0\n",
    "  while(len(dataSet) != 0):\n",
    "    # training data\n",
    "    try:\n",
    "      croppedList = CropData(dataSet, 100)\n",
    "      label, data = LoadVertices(TESTLANDMARKPATH, croppedList)\n",
    "      result = model.evaluate(data, label, batch_size = 1, callbacks =[tensorboard])\n",
    "      lostSum += result[0]\n",
    "      accuracySum += result[1]\n",
    "    except:\n",
    "      print(\"Failed to train data\")\n",
    "\n",
    "    count += 1\n",
    "    reset_keras()\n",
    "  \n",
    "  return lostSum/count, accuracySum/count\n",
    "\n",
    "def DrawFaceLandmark(label, image, landmarks):\n",
    "  mp_drawing = mp.solutions.drawing_utils\n",
    "  mp_drawing_styles = mp.solutions.drawing_styles\n",
    "  mp_face_mesh = mp.solutions.face_mesh\n",
    "  print(f'Face landmarks of {label}:')\n",
    "  if not landmarks.multi_face_landmarks:\n",
    "    print(\"unagle to locate face landmark for {}\".format(label))\n",
    "  annotated_image = image.copy()\n",
    "  for face_landmarks in landmarks.multi_face_landmarks:\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_iris_connections_style())\n",
    "  plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a2898-2d42-418a-9cef-a6cfb6324bfc",
   "metadata": {},
   "source": [
    "## Setup directory and import image file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b1a99c-a616-45b2-b662-fafad60524af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded:10000\n",
      "Files loaded:20000\n",
      "Total images loaded:  24000\n",
      "Images Loaded:  [3000, 3000, 3000, 3000, 3000, 3000, 3000, 3000]\n",
      "Total images loaded:  3987\n",
      "Images Loaded:  [499, 500, 498, 498, 498, 500, 499, 495]\n"
     ]
    }
   ],
   "source": [
    "# Get currect directory (os.getcwd() -> C:\\Users\\jazzt\\src)\n",
    "\n",
    "#-----------------------Start of code---------------------------\n",
    "# initialise image names and label\n",
    "mainTrainSet = LoadAllImageNames(TRAINLANDMARKPATH, catLimit=[3000]*8)\n",
    "mainTestSet = LoadAllImageNames(TESTLANDMARKPATH, catLimit=[500]*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abcfdeee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reset learning rate callback to counter ReduceLROnPlateau\n",
    "class ResetLR(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, learningRate):\n",
    "        self.default_lr = learningRate\n",
    "    def on_train_begin(self, logs={}):\n",
    "        previous_lr = self.model.optimizer.lr.read_value()\n",
    "        if previous_lr != self.default_lr:\n",
    "            print(\"Resetting learning rate from {} to {}\".format(previous_lr, self.default_lr))\n",
    "            self.model.optimizer.lr.assign(self.default_lr)\n",
    "\n",
    "# # making stopper to only start counting from 3 epochs onwards\n",
    "# class CustomStopper(tf.keras.callbacks.EarlyStopping):\n",
    "#     def __init__(self, monitor='val_loss',\n",
    "#              min_delta=0, patience=3, verbose=0, mode='auto', start_epoch = 3): # add argument for starting epoch\n",
    "#         super(CustomStopper, self).__init__()\n",
    "#         self.start_epoch = start_epoch\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         if epoch > self.start_epoch:\n",
    "#             super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8823a64a-7b04-4170-a233-886adc9934ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 27ms/step - loss: 2.1758 - accuracy: 0.1306 - val_loss: 2.0775 - val_accuracy: 0.1366 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0767 - accuracy: 0.1453 - val_loss: 2.0813 - val_accuracy: 0.1317 - lr: 0.0100\n",
      "5.116401195526123\n",
      "trained image count:  1024\n",
      "Images Left:  22976\n",
      "Estimated completion time:  0:01:54.799252\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0840 - accuracy: 0.1074 - val_loss: 2.0761 - val_accuracy: 0.1756 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.0848 - accuracy: 0.1074 - val_loss: 2.0764 - val_accuracy: 0.1366 - lr: 0.0100\n",
      "4.827510356903076\n",
      "trained image count:  2048\n",
      "Images Left:  21952\n",
      "Estimated completion time:  0:01:43.489753\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 29ms/step - loss: 2.0791 - accuracy: 0.1343 - val_loss: 2.0816 - val_accuracy: 0.1366 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 6s 28ms/step - loss: 2.0778 - accuracy: 0.1306 - val_loss: 2.0803 - val_accuracy: 0.1366 - lr: 0.0100\n",
      "5.811670660972595\n",
      "trained image count:  3072\n",
      "Images Left:  20928\n",
      "Estimated completion time:  0:01:58.776019\n",
      "loading image\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 28ms/step - loss: 2.0836 - accuracy: 0.1245 - val_loss: 2.0763 - val_accuracy: 0.1512 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 6s 29ms/step - loss: 2.0813 - accuracy: 0.1429 - val_loss: 2.0763 - val_accuracy: 0.1317 - lr: 0.0100\n",
      "5.901892423629761\n",
      "trained image count:  4096\n",
      "Images Left:  19904\n",
      "Estimated completion time:  0:01:54.718034\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 28ms/step - loss: 2.0828 - accuracy: 0.1270 - val_loss: 2.0726 - val_accuracy: 0.1415 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.0781 - accuracy: 0.1221 - val_loss: 2.0711 - val_accuracy: 0.1415 - lr: 0.0100\n",
      "5.33876645565033\n",
      "trained image count:  5120\n",
      "Images Left:  18880\n",
      "Estimated completion time:  0:01:38.433507\n",
      "loading image\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0839 - accuracy: 0.1282 - val_loss: 2.0833 - val_accuracy: 0.1171 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0799 - accuracy: 0.1368 - val_loss: 2.0882 - val_accuracy: 0.1171 - lr: 0.0100\n",
      "4.676089763641357\n",
      "trained image count:  6144\n",
      "Images Left:  17856\n",
      "Estimated completion time:  0:01:21.539315\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0832 - accuracy: 0.1306 - val_loss: 2.0833 - val_accuracy: 0.1122 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 27ms/step - loss: 2.0796 - accuracy: 0.1355 - val_loss: 2.0847 - val_accuracy: 0.1122 - lr: 0.0100\n",
      "5.064993858337402\n",
      "trained image count:  7168\n",
      "Images Left:  16832\n",
      "Estimated completion time:  0:01:23.255837\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.0833 - accuracy: 0.1136 - val_loss: 2.0891 - val_accuracy: 0.0976 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 22ms/step - loss: 2.0788 - accuracy: 0.1392 - val_loss: 2.0928 - val_accuracy: 0.0976 - lr: 0.0100\n",
      "4.7560049295425415\n",
      "trained image count:  8192\n",
      "Images Left:  15808\n",
      "Estimated completion time:  0:01:13.420826\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 25ms/step - loss: 2.0844 - accuracy: 0.1380 - val_loss: 2.0811 - val_accuracy: 0.1268 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 26ms/step - loss: 2.0822 - accuracy: 0.1306 - val_loss: 2.0832 - val_accuracy: 0.1268 - lr: 0.0100\n",
      "5.261141896247864\n",
      "trained image count:  9216\n",
      "Images Left:  14784\n",
      "Estimated completion time:  0:01:15.957736\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 28ms/step - loss: 2.0806 - accuracy: 0.1197 - val_loss: 2.0895 - val_accuracy: 0.1073 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 26ms/step - loss: 2.0788 - accuracy: 0.1306 - val_loss: 2.0961 - val_accuracy: 0.1073 - lr: 0.0100\n",
      "5.590256333351135\n",
      "trained image count:  10240\n",
      "Images Left:  13760\n",
      "Estimated completion time:  0:01:15.119069\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 28ms/step - loss: 2.0845 - accuracy: 0.1111 - val_loss: 2.0840 - val_accuracy: 0.1073 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 6s 28ms/step - loss: 2.0831 - accuracy: 0.1074 - val_loss: 2.0815 - val_accuracy: 0.1463 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "205/205 [==============================] - 5s 25ms/step - loss: 2.0826 - accuracy: 0.1245 - val_loss: 2.0867 - val_accuracy: 0.1220 - lr: 0.0100\n",
      "5.561906814575195\n",
      "trained image count:  11264\n",
      "Images Left:  12736\n",
      "Estimated completion time:  0:01:09.176216\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0834 - accuracy: 0.1233 - val_loss: 2.0891 - val_accuracy: 0.1122 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 6s 27ms/step - loss: 2.0806 - accuracy: 0.1209 - val_loss: 2.0878 - val_accuracy: 0.1122 - lr: 0.0100\n",
      "5.117534160614014\n",
      "trained image count:  12288\n",
      "Images Left:  11712\n",
      "Estimated completion time:  0:00:58.531797\n",
      "loading image\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 28ms/step - loss: 2.0830 - accuracy: 0.1258 - val_loss: 2.0858 - val_accuracy: 0.1171 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.0790 - accuracy: 0.1221 - val_loss: 2.0956 - val_accuracy: 0.0976 - lr: 0.0100\n",
      "5.391748547554016\n",
      "trained image count:  13312\n",
      "Images Left:  10688\n",
      "Estimated completion time:  0:00:56.276375\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 25ms/step - loss: 2.0857 - accuracy: 0.1172 - val_loss: 2.0754 - val_accuracy: 0.1463 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 2.0820 - accuracy: 0.1392 - val_loss: 2.0759 - val_accuracy: 0.1463 - lr: 0.0100\n",
      "5.595675826072693\n",
      "trained image count:  14336\n",
      "Images Left:  9664\n",
      "Estimated completion time:  0:00:52.809191\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 2.0843 - accuracy: 0.1197 - val_loss: 2.0887 - val_accuracy: 0.1317 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.0823 - accuracy: 0.1258 - val_loss: 2.0903 - val_accuracy: 0.1317 - lr: 0.0100\n",
      "5.5675458908081055\n",
      "trained image count:  15360\n",
      "Images Left:  8640\n",
      "Estimated completion time:  0:00:46.976168\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.0831 - accuracy: 0.1148 - val_loss: 2.0840 - val_accuracy: 0.1073 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 27ms/step - loss: 2.0816 - accuracy: 0.1404 - val_loss: 2.0848 - val_accuracy: 0.1268 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0791 - accuracy: 0.1380 - val_loss: 2.0852 - val_accuracy: 0.1268 - lr: 0.0050\n",
      "5.007043917973836\n",
      "trained image count:  16384\n",
      "Images Left:  7616\n",
      "Estimated completion time:  0:00:37.239889\n",
      "loading image\n",
      "Resetting learning rate from 0.0024999999441206455 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 29ms/step - loss: 2.0845 - accuracy: 0.1197 - val_loss: 2.0834 - val_accuracy: 0.1610 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 6s 28ms/step - loss: 2.0832 - accuracy: 0.1270 - val_loss: 2.0771 - val_accuracy: 0.1610 - lr: 0.0100\n",
      "5.843759775161743\n",
      "trained image count:  17408\n",
      "Images Left:  6592\n",
      "Estimated completion time:  0:00:37.619204\n",
      "loading image\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 29ms/step - loss: 2.0842 - accuracy: 0.1270 - val_loss: 2.0866 - val_accuracy: 0.1024 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0835 - accuracy: 0.1306 - val_loss: 2.0865 - val_accuracy: 0.1024 - lr: 0.0100\n",
      "5.317203879356384\n",
      "trained image count:  18432\n",
      "Images Left:  5568\n",
      "Estimated completion time:  0:00:28.912296\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0828 - accuracy: 0.1404 - val_loss: 2.0790 - val_accuracy: 0.1317 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 25ms/step - loss: 2.0805 - accuracy: 0.1477 - val_loss: 2.0822 - val_accuracy: 0.1317 - lr: 0.0100\n",
      "4.954926013946533\n",
      "trained image count:  19456\n",
      "Images Left:  4544\n",
      "Estimated completion time:  0:00:21.987484\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 31ms/step - loss: 2.0818 - accuracy: 0.1294 - val_loss: 2.0824 - val_accuracy: 0.1512 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 25ms/step - loss: 2.0811 - accuracy: 0.1245 - val_loss: 2.0866 - val_accuracy: 0.1512 - lr: 0.0100\n",
      "5.756299376487732\n",
      "trained image count:  20480\n",
      "Images Left:  3520\n",
      "Estimated completion time:  0:00:19.787279\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 25ms/step - loss: 2.0833 - accuracy: 0.1233 - val_loss: 2.0823 - val_accuracy: 0.1415 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 6s 30ms/step - loss: 2.0819 - accuracy: 0.1306 - val_loss: 2.0812 - val_accuracy: 0.1415 - lr: 0.0100\n",
      "5.617164373397827\n",
      "trained image count:  21504\n",
      "Images Left:  2496\n",
      "Estimated completion time:  0:00:13.691838\n",
      "loading image\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 29ms/step - loss: 2.0802 - accuracy: 0.1368 - val_loss: 2.0822 - val_accuracy: 0.1122 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 25ms/step - loss: 2.0801 - accuracy: 0.1306 - val_loss: 2.0838 - val_accuracy: 0.1122 - lr: 0.0100\n",
      "5.547065258026123\n",
      "trained image count:  22528\n",
      "Images Left:  1472\n",
      "Estimated completion time:  0:00:07.973906\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.0827 - accuracy: 0.1319 - val_loss: 2.0812 - val_accuracy: 0.1024 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 6s 29ms/step - loss: 2.0795 - accuracy: 0.1343 - val_loss: 2.0865 - val_accuracy: 0.1024 - lr: 0.0100\n",
      "5.398186683654785\n",
      "trained image count:  23552\n",
      "Images Left:  448\n",
      "Estimated completion time:  0:00:02.361707\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 2s 25ms/step - loss: 2.0836 - accuracy: 0.1313 - val_loss: 2.0779 - val_accuracy: 0.1333 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 2s 22ms/step - loss: 2.0848 - accuracy: 0.1173 - val_loss: 2.0813 - val_accuracy: 0.1000 - lr: 0.0100\n",
      "2.164018750190735\n",
      "trained image count:  24000\n",
      "Images Left:  0\n",
      "Estimated completion time:  0:00:00\n",
      "---------------- freeze_100epoch_0.01lr_4bs_512fls_64sls_landmark Training Completed ------------------\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0906 - accuracy: 0.1000\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0749 - accuracy: 0.1400\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0835 - accuracy: 0.1100\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 2.0836 - accuracy: 0.1100\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0832 - accuracy: 0.1300\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 2.0806 - accuracy: 0.1300\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0884 - accuracy: 0.1100\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0856 - accuracy: 0.0900\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 2.0852 - accuracy: 0.0800\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 2.0849 - accuracy: 0.0900\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 2.0810 - accuracy: 0.1400\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0796 - accuracy: 0.1100\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0797 - accuracy: 0.1500\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0808 - accuracy: 0.0800\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0839 - accuracy: 0.1300\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 2.0936 - accuracy: 0.0700\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0776 - accuracy: 0.1500\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0790 - accuracy: 0.1100\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 2.0735 - accuracy: 0.1700\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 2.0723 - accuracy: 0.1600\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0793 - accuracy: 0.1500\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0818 - accuracy: 0.1000\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0750 - accuracy: 0.1500\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0723 - accuracy: 0.1800\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0798 - accuracy: 0.1400\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0869 - accuracy: 0.0900\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 2.0780 - accuracy: 0.1200\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0852 - accuracy: 0.0900\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 2.0786 - accuracy: 0.1000\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0776 - accuracy: 0.1600\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.0742 - accuracy: 0.1600\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 2.0707 - accuracy: 0.2200\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0815 - accuracy: 0.1300\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0889 - accuracy: 0.0600\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 2.0804 - accuracy: 0.0900\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0793 - accuracy: 0.1300\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0824 - accuracy: 0.1300\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 2.0775 - accuracy: 0.1500\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.0853 - accuracy: 0.0800\n",
      "87/87 [==============================] - 3s 35ms/step - loss: 2.0826 - accuracy: 0.1839\n",
      "==========FINISH TESTING===========\n",
      "\n",
      "  model name: freeze_100epoch_0.01lr_4bs_512fls_64sls_landmark\n",
      "  average lost: 2.0809724807739256\n",
      "  average accuracy: 0.12434770138934255\n",
      "            \n",
      "loading image\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 27ms/step - loss: 2.2479 - accuracy: 0.1099 - val_loss: 2.0809 - val_accuracy: 0.1317 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 27ms/step - loss: 2.0750 - accuracy: 0.1453 - val_loss: 2.0832 - val_accuracy: 0.1317 - lr: 0.0100\n",
      "5.4552857875823975\n",
      "trained image count:  1024\n",
      "Images Left:  22976\n",
      "Estimated completion time:  0:02:02.402975\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 25ms/step - loss: 2.0849 - accuracy: 0.1209 - val_loss: 2.0800 - val_accuracy: 0.0927 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 4s 21ms/step - loss: 2.0851 - accuracy: 0.1270 - val_loss: 2.0739 - val_accuracy: 0.1463 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "205/205 [==============================] - 4s 21ms/step - loss: 2.0824 - accuracy: 0.1258 - val_loss: 2.0787 - val_accuracy: 0.0927 - lr: 0.0100\n",
      "4.571032603581746\n",
      "trained image count:  2048\n",
      "Images Left:  21952\n",
      "Estimated completion time:  0:01:37.991511\n",
      "loading image\n",
      "Resetting learning rate from 0.004999999888241291 to 0.01\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0806 - accuracy: 0.1245 - val_loss: 2.0843 - val_accuracy: 0.1366 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 4s 20ms/step - loss: 2.0773 - accuracy: 0.1355 - val_loss: 2.0840 - val_accuracy: 0.1366 - lr: 0.0100\n",
      "4.479668974876404\n",
      "trained image count:  3072\n",
      "Images Left:  20928\n",
      "Estimated completion time:  0:01:31.553235\n",
      "loading image\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 6s 28ms/step - loss: 2.0833 - accuracy: 0.1111 - val_loss: 2.0762 - val_accuracy: 0.1317 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0812 - accuracy: 0.1343 - val_loss: 2.0756 - val_accuracy: 0.1317 - lr: 0.0100\n",
      "5.257646560668945\n",
      "trained image count:  4096\n",
      "Images Left:  19904\n",
      "Estimated completion time:  0:01:42.195505\n",
      "loading image\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0803 - accuracy: 0.1355 - val_loss: 2.0716 - val_accuracy: 0.1366 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "205/205 [==============================] - 5s 23ms/step - loss: 2.0778 - accuracy: 0.1368 - val_loss: 2.0696 - val_accuracy: 0.1415 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "205/205 [==============================] - 5s 24ms/step - loss: 2.0772 - accuracy: 0.1343 - val_loss: 2.0691 - val_accuracy: 0.1415 - lr: 0.0100\n",
      "4.811651070912679\n",
      "trained image count:  5120\n",
      "Images Left:  18880\n",
      "Estimated completion time:  0:01:28.714817\n",
      "loading image\n",
      "Epoch 1/100\n",
      "205/205 [==============================] - 5s 22ms/step - loss: 2.0847 - accuracy: 0.1099 - val_loss: 2.0870 - val_accuracy: 0.1171 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "164/205 [=======================>......] - ETA: 0s - loss: 2.0812 - accuracy: 0.1265"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18008\\1467842098.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatchSize\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatchSizes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mfirstLayerSize\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfirstLayerSizes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msecondLayerSize\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msecondLayerSizes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"freeze_{}epoch_{}lr_{}bs_{}fls_{}sls_landmark\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirstLayerSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecondLayerSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m       \u001b[0mFreezeLayerTrainAndTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirstLayerSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecondLayerSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18008\\3380140027.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(epoch, learningRate, modelName, batchSize, firstLayerSize, secondLayerSize)\u001b[0m\n\u001b[0;32m     41\u001b[0m       \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoadVertices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAINLANDMARKPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcroppedList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m       \u001b[1;31m# Training model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtime_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearlyStop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearlyStopAcc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_lr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_callback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAverageTime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# Saving weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1380\u001b[0m                 \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m   1019\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[1;34m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1006\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         run_step = tf.function(\n\u001b[0;32m   1008\u001b[0m             run_step, jit_compile=True, experimental_relax_shapes=True)\n\u001b[0;32m   1009\u001b[0m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m       outputs = reduce_per_replica(\n\u001b[0;32m   1012\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1308\u001b[0m       \u001b[1;31m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m       \u001b[1;31m# applied when the caller is also in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m       fn = autograph.tf_convert(\n\u001b[0;32m   1311\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m-> 1312\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2884\u001b[0m     \u001b[0m_require_cross_replica_or_default_context_extended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2885\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2886\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2887\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2888\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3687\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3688\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3689\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1000\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1001\u001b[0m         \u001b[1;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    856\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_x_y_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m     \u001b[1;31m# Run forward pass.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m       \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_target_and_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[1;31m# Run backwards pass.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    914\u001b[0m       The total loss as a `tf.Tensor`, or `None` if no loss results (which is\n\u001b[0;32m    915\u001b[0m       the case when called by `Model.test_step`).\n\u001b[0;32m    916\u001b[0m     \"\"\"\n\u001b[0;32m    917\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mx\u001b[0m  \u001b[1;31m# The default implementation does not use `x`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     return self.compiled_loss(\n\u001b[0m\u001b[0;32m    919\u001b[0m         y, y_pred, sample_weight, regularization_losses=self.losses)\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\engine\\compile_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloss_values\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m       loss_metric_values = losses_utils.cast_losses_to_common_dtype(\n\u001b[0;32m    237\u001b[0m           loss_metric_values)\n\u001b[0;32m    238\u001b[0m       \u001b[0mtotal_loss_metric_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_metric_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m       self._loss_metric.update_state(\n\u001b[0m\u001b[0;32m    240\u001b[0m           total_loss_metric_value, sample_weight=batch_dim)\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m       \u001b[0mloss_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_losses_to_common_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\utils\\metrics_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;34m'the metric was not created in TPUStrategy scope. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             'Make sure the keras Metric is created in TPUstrategy scope. ')\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# update_op will be None in eager execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\metrics.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mcontrol_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mag_update_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_update_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m           optional_features=optional_features)\n\u001b[0;32m    687\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaller_fn_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallopts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Allowlisted %s: from cache'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Allowlisted: %s: AutoGraph is disabled in context'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    454\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__self__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfMethodTarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__self__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\keras\\metrics.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, values, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m         msg += ('To return a dict of values, implement a custom Metric '\n\u001b[0;32m    446\u001b[0m                 'subclass.')\n\u001b[0;32m    447\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m       \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m       \u001b[1;31m# Update dimensions of weights to match with values if possible.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m       values, _, sample_weight = losses_utils.squeeze_or_expand_dimensions(\n\u001b[0;32m    452\u001b[0m           values, sample_weight=sample_weight)\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1083\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, dtype, name)\u001b[0m\n\u001b[0;32m    998\u001b[0m       \u001b[1;31m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[1;31m# strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1003\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\tflite-facial-expression\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[0;32m   1997\u001b[0m         _ctx, \"Cast\", name, x, \"DstT\", DstT, \"Truncate\", Truncate)\n\u001b[0;32m   1998\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1999\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2001\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2002\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2003\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2004\u001b[0m       return cast_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batchSizes = [4, 16, 32, 48, 64, 96]\n",
    "firstLayerSizes = [512, 764, 1024, 1536, 2048]\n",
    "secondLayerSizes = [64, 128, 192, 256, 384]\n",
    "epoch = 13\n",
    "learningRate = 0.05\n",
    "\n",
    "for batchSize in batchSizes:\n",
    "  for firstLayerSize in firstLayerSizes:\n",
    "    for secondLayerSize in secondLayerSizes:\n",
    "      name = \"freeze_{}epoch_{}lr_{}bs_{}fls_{}sls_landmark\".format(epoch, learningRate, batchSize, firstLayerSize, secondLayerSize)\n",
    "      FreezeLayerTrainAndTest(epoch, learningRate, name, batchSize, firstLayerSize, secondLayerSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0af04fac-a41d-4009-ac2b-3e838fe4c778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FreezeLayerTrainAndTest(epoch, learningRate, modelName, batchSize, firstLayerSize, secondLayerSize):\n",
    "  \"\"\"Train and test a model. This function's input should be customized to accept different parameters, function body should be customized to these parameters to train models of variety of paramters.\n",
    "\n",
    "  Args:\n",
    "      modelName (str): name of model\n",
    "  \"\"\"\n",
    "  #-------------------------------------------------\n",
    "  # Building model\n",
    "  #-------------------------------------------------\n",
    "  # Adding 3 more layers to output side\n",
    "  inputs = tf.keras.Input(shape=(478,3))\n",
    "  base = tf.keras.layers.Dense(firstLayerSize, activation='relu')(inputs)\n",
    "  base = tf.keras.layers.Dense(secondLayerSize, activation='relu')(base)\n",
    "  base = tf.keras.layers.Flatten()(base)\n",
    "  outputs = tf.keras.layers.Dense(8, activation='softmax')(base)\n",
    "  \n",
    "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = keras.optimizers.Adam(lr = learningRate), metrics=[\"accuracy\"])\n",
    "\n",
    "  # ------------------ Callbacks ------------------\n",
    "  tensorboard = TensorBoard(log_dir = \"logs/{}\".format(modelName))\n",
    "  earlyStop = EarlyStopping(monitor='val_loss', patience = 4)\n",
    "  earlyStopAcc = EarlyStopping(monitor='val_accuracy', patience = 1, restore_best_weights = True)\n",
    "  reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 1, min_lr = 0.001)\n",
    "  reset_lr = ResetLR(learningRate)\n",
    "\n",
    "  # ------------------ Training Model ------------------\n",
    "  time_callback = TimeHistory()\n",
    "  trainDataSet = mainTrainSet.copy()\n",
    "\n",
    "  # Initializing model fit params\n",
    "  # batchSize = 4\n",
    "  imgPerIter = batchSize*256\n",
    "\n",
    "  count = 0\n",
    "  while(len(trainDataSet) != 0):\n",
    "    croppedList = []\n",
    "    try:\n",
    "      print(\"loading image\") # Crop and load images in\n",
    "      croppedList = CropData(trainDataSet, imgPerIter)\n",
    "      label, data = LoadVertices(TRAINLANDMARKPATH, croppedList)\n",
    "      # Training model\n",
    "      model.fit(data, label, epochs = epoch, validation_split = 0.2, batch_size = batchSize, callbacks = [time_callback, tensorboard, earlyStop, earlyStopAcc, reduce_lr, reset_lr])\n",
    "      print(time_callback.AverageTime())\n",
    "    except RuntimeError as e:\n",
    "      print(e)\n",
    "    \n",
    "    # Saving weights\n",
    "    model.save_weights('{}.h5'.format(modelName))\n",
    "    \n",
    "    # Print summary of current iteration\n",
    "    count = count + len(croppedList)\n",
    "    timeLeft = len(trainDataSet)/imgPerIter * time_callback.AverageTime()\n",
    "    print(\"trained image count: \", count)\n",
    "    print(\"Images Left: \", len(trainDataSet))\n",
    "    print(\"Estimated completion time: \", str(timedelta(seconds=timeLeft)))\n",
    "    \n",
    "    reset_keras()\n",
    "  print(\"---------------- {} Training Completed ------------------\".format(modelName))\n",
    "\n",
    "  # ---------------- Testing Model -----------------\n",
    "  # initialise image names and label\n",
    "  testSetData = mainTestSet.copy()\n",
    "\n",
    "  # test model\n",
    "  loss, accuracy = TestModelWithVertices(model, testSetData, tensorboard)\n",
    "\n",
    "  # Saving test informations\n",
    "  modelSummary = '''\n",
    "  model name: {}\n",
    "  average lost: {}\n",
    "  average accuracy: {}\n",
    "            '''.format(modelName, loss, accuracy)\n",
    "      \n",
    "  print(\"==========FINISH TESTING===========\\n{}\".format(modelSummary))\n",
    "  \n",
    "  file = open(\"training_summaries.txt\", \"a\")\n",
    "  file.write(modelSummary + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f35c1-9369-4052-88c4-e38a5146dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b9aab-4d6b-4547-9f7b-fc718d857e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
