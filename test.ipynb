{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ae7ad7-56bc-404b-8c5c-097efec93913",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is where it all began. Initially this was the main notebook to do neural network training and testing, now it's a repository of history of work, ranging from loading of images, printing images, compiling, training and testing of neural network with CNN model, tuning GPU Setting, utilizing mediapipe for data wrangling, using tensorboard for analysis of hyperparameter tuning, and confusion matrix printing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a214feb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libraries and set up functions\n",
    "Starting directory is this \"..\\\\asset\" from this notebook code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75ea4e-7044-4150-b59e-d3a38ad33473",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from keras.backend import clear_session\n",
    "from keras.backend import get_session\n",
    "import time\n",
    "import math\n",
    "from datetime import timedelta\n",
    "import seaborn as sns \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# import mediapipe as mp\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"..\\\\asset\") # change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0796a-7779-40f1-a077-c013999ee3e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "emotionList = ('Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt')\n",
    "TRAINANNOTATIONPATH = \"train_set\\\\annotations\\\\\"\n",
    "TRAINIMAGEPATH = \"train_set\\\\images\\\\\"\n",
    "TRAINLANDMARKPATH = \"train_set\\\\landmarks\\\\\"\n",
    "TESTANNOTATIONPATH = \"val_set\\\\annotations\\\\\"\n",
    "TESTIMAGEPATH = \"val_set\\\\images\\\\\"\n",
    "TESTLANDMARKPATH = \"val_set\\\\landmarks\\\\\"\n",
    "\n",
    "# Helper functions (Some are relevant to this notebook)\n",
    "\n",
    "def LoadAllImageNames(filePath, limit = 0, catLimit = []):\n",
    "  \"\"\"Load the images names and label in tuple format (label, image name)\n",
    "\n",
    "  Args:\n",
    "      filePath (str): directory of image folder\n",
    "      limit (int, optional): max number of image to load. Defaults to 0.\n",
    "      catLimit (list<int>, optional): array of image count limit for each class. Defaults to []*8.\n",
    "\n",
    "  Returns:\n",
    "      list<str>: shuffled list of image names\n",
    "  \"\"\"\n",
    "  limitCounter = [0,0,0,0,0,0,0,0]\n",
    "  dataSet = []\n",
    "  loadCounter = 0\n",
    "\n",
    "  annotationPath = TRAINANNOTATIONPATH if (\"train\" in filePath) else TESTANNOTATIONPATH\n",
    "  for file in os.scandir(filePath):\n",
    "    if (limit > 0 and loadCounter > limit):\n",
    "      break\n",
    "  \n",
    "    name = file.name[:-4] # file name w/o file extension\n",
    "    if \"_landmark\" in name:\n",
    "      name = name[:-9]\n",
    "    data = np.load(\"{}{}_exp.npy\".format(annotationPath, name)) # \n",
    "    label = int(data.item(0))\n",
    "\n",
    "    if len(catLimit) > 0 and limitCounter[label] >= catLimit[label]:\n",
    "        continue\n",
    "    limitCounter[label] += 1\n",
    "    loadCounter += 1\n",
    "\n",
    "    dataSet.append((label, file.name))\n",
    "    if (loadCounter%10000==0):\n",
    "      print(\"Files loaded:{}\".format(loadCounter))\n",
    "  \n",
    "  print(\"Total images loaded: \", loadCounter)\n",
    "  print(\"Images Loaded: \", limitCounter)\n",
    "  random.shuffle(dataSet)\n",
    "  return dataSet\n",
    "\n",
    "# Load the pixels of a picture to numpy.ndarray format. false for test set, true for training set\n",
    "# Return image in RGB format\n",
    "def LoadImage(imagePath, imageName, normalize = True):\n",
    "  \"\"\"Load image using numpy\n",
    "\n",
    "  Args:\n",
    "      imagePath (str): image path \n",
    "      imageName (str): image name\n",
    "      normalize (bool, optional): To normalize image or not. Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "      numpy array: x,y,3 array\n",
    "  \"\"\"\n",
    "  # print(\"{}{}{}\".format(os.getcwd(), \"\\\\\"+imagePath, imageName))\n",
    "  image_array = cv2.imread(\"{}{}{}\".format(os.getcwd(), \"\\\\\"+imagePath, imageName))\n",
    "  image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
    "  if normalize:\n",
    "    image_array = image_array/255\n",
    "    \n",
    "  return image_array\n",
    "\n",
    "# Extract the data from 0 to amount from list and return it\n",
    "def CropData(list, amount):\n",
    "  if (len(list) < amount):\n",
    "    amount = len(list)\n",
    "  croppedList = list[:amount]\n",
    "  del list[:amount]\n",
    "  return croppedList\n",
    "\n",
    "# Load image from a list of image names and a selected path, return a list of the label and image data\n",
    "def LoadImages(imagePath, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entries in list:\n",
    "    try:\n",
    "      image = LoadImage(imagePath, entries[1])\n",
    "      data.append(image) \n",
    "      label.append(entries[0]) \n",
    "    except: \n",
    "      print(\"Failed to load training image: \", entries[1])\n",
    "  npLabel = np.array(label) \n",
    "  npData = np.array(data) \n",
    "  return npLabel, npData \n",
    "\n",
    "# Load a list of face mesh in landmark_pb2.NormalizedLandmarkList format, return the list of label and list of vertices data loaded\n",
    "def LoadFaceMeshes(faceMesh, path, list, withFace = False):\n",
    "  face = []\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    # try:\n",
    "    if withFace:\n",
    "      face.append(LoadImage(path, entry[1], normalize = False)) # Load image\n",
    "      data.append(faceMesh.process(face[-1])) # process it into face mesh and append into data\n",
    "    else:\n",
    "      data.append(faceMesh.process(LoadImage(path, entry[1], normalize = False))) # Load image, process it into face mesh, and append into data\n",
    "    label.append(entry[0])\n",
    "    # except:\n",
    "      # print(\"Failed to load training image: \", entry[1])\n",
    "\n",
    "  if withFace:\n",
    "    return np.array(label), np.array(data), np.array(face)\n",
    "  else:\n",
    "    return np.array(label), np.array(data)\n",
    "\n",
    "# Using mediapipe to convert a list of images to face mesh vertices and return the list of labels and list of vertices\n",
    "def LoadFaceMeshVertices(faceMesh, path, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    # load image, process it into normalizedlandmarklist format, obtain face data from the list, convert it into vertices\n",
    "    image = LoadImage(path, entry[1], normalize = False)\n",
    "    multiFaceLandmarks = faceMesh.process(image).multi_face_landmarks\n",
    "    if (multiFaceLandmarks == None):\n",
    "      print(\"{} multi-face landmarks is none\".format(entry[1]))\n",
    "      continue\n",
    "    vertices = MultiLandmarkToVertices(multiFaceLandmarks[0])\n",
    "    data.append(vertices)\n",
    "    label.append(entry[0])\n",
    "    # except:\n",
    "      # print(\"Failed to load training image: \", entry[1])\n",
    "\n",
    "# Load in a list of face vertices and return the list of label and list of vertices\n",
    "def LoadVertices(path, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    vertices = np.load(\"{}{}{}.npy\".format(os.getcwd(), \"\\\\\"+path, entry[1][:-4]))\n",
    "    data.append(vertices)\n",
    "    label.append(entry[0])\n",
    "  \n",
    "  return np.array(label), np.array(data)\n",
    "\n",
    "# Convert a list of mediapipe's multilandmark structure into vertices and return it\n",
    "def MultiLandmarkToVertices(multiLandmark):\n",
    "  verticesList = []\n",
    "  for idx, landmark in enumerate(multiLandmark.landmark):\n",
    "    verticesList.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "  return verticesList\n",
    "\n",
    "# Initialise a CNN-DNN model and return it\n",
    "def InitializeModel():\n",
    "  pretrained_model = tf.keras.applications.MobileNetV3Large(input_shape=(224,224,3)) # Initializing model with mobile net V3 pretrained model\n",
    "\n",
    "  # Initializing the input and output from the model, removing last layer\n",
    "  base_input = pretrained_model.layers[0].input\n",
    "  base_output = pretrained_model.layers[-2].output\n",
    "\n",
    "  # Adding 3 more layers to output side\n",
    "  final_output = layers.Dense(128)(base_output) # Adding new layers, to the output side\n",
    "  final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "  final_output = layers.Dense(64)(final_output)\n",
    "  final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "  final_output = layers.Dense(8, activation = 'softmax')(final_output) # 8 cuz there are 8 image classifications\n",
    "\n",
    "  new_model = keras.Model(inputs = base_input, outputs = final_output)\n",
    "  # new_model.summary()\n",
    "  return new_model\n",
    "\n",
    "# Convert rgb image to gray scale\n",
    "def ConvertToGray(image):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "  return image\n",
    "\n",
    "# Scale a image up to specified width size\n",
    "def ScaleImage(image, width):\n",
    "  ratio = image.shape[1]/width\n",
    "  image = cv2.resize(image, (width, int(image.shape[0]/ratio)))\n",
    "  return image\n",
    "\n",
    "# Detect a face in a picture using haarcascade algorithm\n",
    "def DetectFace(image):\n",
    "  face_roi = np.ndarray(1)\n",
    "  faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "  grayImage = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  faces = faceCascade.detectMultiScale(grayImage, 1.3, 5)\n",
    "  for x,y,w,h in faces:\n",
    "    roi_gray = grayImage[y:y+h, x:x+w]\n",
    "    roi_color = image[y:y+h, x:x+w]\n",
    "    cv2.rectangle(image, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "    facess = faceCascade.detectMultiScale(roi_gray)\n",
    "    if (len(facess) == 0):\n",
    "      print(\"Face not detected\")\n",
    "    else:\n",
    "      for (ex,ey,ew,eh) in facess:\n",
    "        face_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "  return face_roi\n",
    "\n",
    "# Convert an image to a numpy array input\n",
    "def ConvertToInput(image):\n",
    "  input = ScaleImage(image, 224)\n",
    "  input = np.expand_dims(input, axis = 0) ## to add fourth dimension to fit model input\n",
    "  input = input/255\n",
    "  return input\n",
    "\n",
    "# Predict the result of a single image\n",
    "def GetResult(model, input):\n",
    "  Predictions = model.predict(input)\n",
    "  print(Predictions)\n",
    "  result = np.argmax(Predictions)\n",
    "  return emotionList[result]\n",
    "\n",
    "# Takes in a list of dataset with labels and names, then do a count on each label and return the counting result\n",
    "def printDataSetLabels(dataSet):\n",
    "    counterList = list(range(8))\n",
    "    for label, name in dataSet:\n",
    "        counterList[label] += 1\n",
    "    print(counterList)\n",
    "    \n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    # try:\n",
    "    #     del classifier # this is from global space - change this as you need\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.9, visible_device_list=\"0\")\n",
    "\n",
    "# callback for recording the time history for tensorboard operation, specifically to calculate the average time taken to run a single epoch\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.times = []\n",
    "\n",
    "  def on_epoch_begin(self, batch, logs={}):\n",
    "    self.epoch_time_start = time.time()\n",
    "\n",
    "  def on_epoch_end(self, batch, logs={}):\n",
    "    self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "  def AverageTime(self):\n",
    "    sum = 0\n",
    "    for time in self.times:\n",
    "      sum += time\n",
    "    return sum/len(self.times)\n",
    "\n",
    "# Takes in a model and a test data set, then perform evaluation on the model and return the average loss and average accuracy\n",
    "def TestModel(model, testSetData):\n",
    "  # test model\n",
    "  lostSum = 0\n",
    "  accuracySum = 0\n",
    "  count = 0\n",
    "  while(len(testSetData) != 0):\n",
    "    # training data\n",
    "    try:\n",
    "      croppedList = CropData(testSetData, 100)\n",
    "      label, data = LoadImages(TESTIMAGEPATH, croppedList)\n",
    "      result = model.evaluate(data, label, batch_size = 1)\n",
    "      lostSum += result[0]\n",
    "      accuracySum += result[1]\n",
    "    except:\n",
    "      print(\"Failed to train data\")\n",
    "\n",
    "    count += 1\n",
    "    reset_keras()\n",
    "  \n",
    "  return lostSum/count, accuracySum/count\n",
    "\n",
    "# Test a model with vertices\n",
    "def TestModelWithVertices(model, dataSet, tensorboard):\n",
    "  # test model\n",
    "  lostSum = 0\n",
    "  accuracySum = 0\n",
    "  count = 0\n",
    "  while(len(dataSet) != 0):\n",
    "    # training data\n",
    "    try:\n",
    "      croppedList = CropData(dataSet, 10000)\n",
    "      label, data = LoadVertices(TESTLANDMARKPATH, croppedList)\n",
    "      result = model.evaluate(data, label, batch_size = 1, callbacks =[tensorboard])\n",
    "      lostSum += result[0]\n",
    "      accuracySum += result[1]\n",
    "    except:\n",
    "      print(\"Failed to train data\")\n",
    "\n",
    "    count += 1\n",
    "    reset_keras()\n",
    "  \n",
    "  return lostSum/count, accuracySum/count\n",
    "\n",
    "# Draw the facial landmark of a face overlaying the actual face\n",
    "def DrawFaceLandmark(label, image, landmarks):\n",
    "  mp_drawing = mp.solutions.drawing_utils\n",
    "  mp_drawing_styles = mp.solutions.drawing_styles\n",
    "  mp_face_mesh = mp.solutions.face_mesh\n",
    "  print(f'Face landmarks of {label}:')\n",
    "  if not landmarks.multi_face_landmarks:\n",
    "    print(\"unagle to locate face landmark for {}\".format(label))\n",
    "  annotated_image = image.copy()\n",
    "  for face_landmarks in landmarks.multi_face_landmarks:\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_iris_connections_style())\n",
    "  plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac7c28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d08f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup directory and import image file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8075535-75ed-424a-b01b-2689f93c99b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get currect directory (os.getcwd() -> C:\\Users\\jazzt\\src)\n",
    "\n",
    "#-----------------------Start of code---------------------------\n",
    "# initialise image names and label\n",
    "# mainTrainSet = LoadAllImageNames(TRAINIMAGEPATH, 80, catLimit=[10]*8) #catLimit=[20]*8)\n",
    "mainTestSet = LoadAllImageNames(TESTLANDMARKPATH, catLimit=[500]*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632f15b-ab4b-434a-b87c-f60ed1c3f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainTestSet = LoadAllImageNames(TESTIMAGEPATH, catLimit=[500]*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0bdc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Compiling and training model\n",
    "Using pretrained mode from MobileNetV3, fine-tuning of model can be done in the first block.\n",
    "In the second block, training of model can be done while tweaking the training parameters such as batch size and epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399c8d4-9ffe-438b-bf76-2cc05f3e34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.MobileNetV3Small(input_shape=(224,224,3),) # Initializing model with mobile net V3 pretrained model\n",
    "\n",
    "pretrained_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d25910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.MobileNetV3Small(input_shape=(224,224,3),) # Initializing model with mobile net V3 pretrained model\n",
    "\n",
    "# Initializing the input and output from the model, removing last layer\n",
    "base_input = pretrained_model.layers[0].input\n",
    "base_output = pretrained_model.layers[-2].output\n",
    "\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "  assert layer.trainable == False\n",
    "  layer.trainable = False\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Customizing layers\n",
    "#-------------------------------------------------\n",
    "# Adding 3 more layers to output side\n",
    "final_output = layers.Dense(256)(base_output) # Adding new layers, to the output side\n",
    "final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "final_output = layers.Dense(64)(final_output) # Adding new layers, to the output side\n",
    "final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "final_output = layers.Dense(8, activation = 'softmax')(final_output) # 8 cuz there are 8 image classifications\n",
    "\n",
    "model = keras.Model(inputs = base_input, outputs = final_output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099e6e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "time_callback = TimeHistory()\n",
    "dataSet = mainTrainSet.copy()\n",
    "\n",
    "# Initializing model fit params\n",
    "batchSize = 8\n",
    "imgPerIter = batchSize*16\n",
    "\n",
    "count = 0\n",
    "while(len(dataSet) != 0):\n",
    "  croppedList = []\n",
    "  try:\n",
    "    print(\"loading image\") # Crop and load images in\n",
    "    croppedList = CropData(dataSet, imgPerIter)\n",
    "    label, data = LoadImages(TRAINIMAGEPATH, croppedList)\n",
    "    \n",
    "    # Training model\n",
    "    model.fit(data, label, epochs = 11, batch_size = batchSize, callbacks = [time_callback])\n",
    "    print(time_callback.AverageTime())\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "  \n",
    "  # Saving weights\n",
    "  model.save_weights('freeze_bs48_512fls_128sls.h5')\n",
    "  \n",
    "  # Print summary of current iteration\n",
    "  count = count + len(croppedList)\n",
    "  timeLeft = len(dataSet)/imgPerIter * time_callback.AverageTime()\n",
    "  print(\"trained image count: \", count)\n",
    "  print(\"Images Left: \", len(dataSet))\n",
    "  print(\"Estimated completion time: \", str(timedelta(seconds=timeLeft)))\n",
    "  \n",
    "  reset_keras()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f04772",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing a random image with a face\n",
    "The following block of code loads a image, crop the image to just the face, and convert it into model input before inserting it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc8dd0-074a-42dd-ae64-4e395ab45f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing trained model\n",
    "image = LoadImage(TESTIMAGEPATH,\"3.jpg\", normalize = False)\n",
    "# image = cv2.resize(image, (224,224))\n",
    "# image = ScaleImage(image, 224)\n",
    "\n",
    "face = DetectFace(image)\n",
    "\n",
    "plt.imshow(face)\n",
    "\n",
    "preppedInput = ConvertToInput(face)\n",
    "result = GetResult(model, preppedInput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a660c-2224-497d-a4b5-e06cbf4ae544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing loading in data from tar using tensorflow\n",
    "# os.chdir(\"C:\\\\Users\\\\j.teoh\\\\Desktop\\\\tflite-facial-expression\") # change working directory\n",
    "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0dff4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test Model\n",
    "The following block of code takes in a bunch of weight names and test their accuracy and lose before printing the result out.\n",
    "Insert the weight file names into \"modelNames\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c5782-12fa-4d90-9fac-f6a3ff6f1d4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Testing model\n",
    "\n",
    "modelNames = [\"Limit30000Label126BatchSize48gpu.h5\"]\n",
    "model = InitializeModel()\n",
    "mainData = LoadAllImageNames(TESTIMAGEPATH)\n",
    "resultPool =[]\n",
    "\n",
    "for modelName in modelNames:\n",
    "    model.load_weights(modelName)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # initialise image names and label\n",
    "    testSetData = []\n",
    "    testSetData.extend(mainData)\n",
    "    # print(mainData)\n",
    "    # test model\n",
    "    lostSum = 0\n",
    "    accuracySum = 0\n",
    "    count = 0\n",
    "    while(len(testSetData) != 0):\n",
    "      # training data\n",
    "      try:\n",
    "        croppedList = CropData(testSetData, 100)\n",
    "        print(\"loading image\")\n",
    "        label, data = LoadImages(TESTIMAGEPATH, croppedList)\n",
    "        result = model.evaluate(data, label, batch_size = 1)\n",
    "        lostSum += result[0]\n",
    "        accuracySum += result[1]\n",
    "      except:\n",
    "        print(\"Failed to train data\")\n",
    "\n",
    "      count += 1\n",
    "      reset_keras()\n",
    "      # print(count)\n",
    "    # print(mainData)\n",
    "    print(\"==========FINISH TESTING===========\")\n",
    "    print(\"model name: \", modelName)\n",
    "    print(\"average lost: \", lostSum/count)\n",
    "    print(\"average accuracy: \", accuracySum/count)\n",
    "    resultPool.append((modelName, lostSum/count, accuracySum/count))\n",
    "    \n",
    "print(resultPool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d07458",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting up image with Keras for MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe1de8-8627-440c-9cab-7626a70eb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "# preparing dataset of 1000 images\n",
    "dataset = trainingSetData[:1000].copy()\n",
    "trainingSet = []\n",
    "for label, fileName in dataset:\n",
    "    # Load image and convert to 224,224,3 nested array\n",
    "    tempImg = image.load_img(\"train_set\\\\images\\\\\"+fileName, target_size = (224,224))\n",
    "    tempImg = image.img_to_array(tempImg)\n",
    "    \n",
    "    # turn all the values in nested array into value between -1 and 1\n",
    "    trainingSet.append(preprocess_input(tempImg))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84796467",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## GPU Setting\n",
    "Uncomment the first line of code to force the kernel to compute with CPU instead of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    # tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # config = tf.compat.v1.ConfigProto()\n",
    "    # config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "    # keras.set_session(tf.Session(config=config))\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bd416-877c-403e-8fa2-f3356d68de0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mediapipe Testings\n",
    "The first block of code loads in train and test images. Then convert them into 468 vertices by batches and save them to landmark folder\n",
    "Prerequisite: Run the import file name code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e74992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this code load a list of face data in facemesh vertices and save it\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True)\n",
    "faceMeshDataSet = []\n",
    "imagePerIter = 100\n",
    "\n",
    "#------ For train images ------\n",
    "faceMeshDataSet.extend(mainTrainSet)\n",
    "\n",
    "while len(faceMeshDataSet) > 0:\n",
    "    croppedList = CropData(faceMeshDataSet, imagePerIter)\n",
    "    label, landmarkVertices = LoadFaceMeshVertices(face_mesh, TRAINIMAGEPATH, croppedList)\n",
    "\n",
    "    count = 0\n",
    "    for name, vertices in zip(croppedList, landmarkVertices):\n",
    "        count += 1\n",
    "        np.save(\"{}{}_landmark\".format(TRAINLANDMARKPATH, name[1][:-4]), vertices)\n",
    "\n",
    "    print(\"{} train landmark converted and saved\".format(count))\n",
    "faceMeshDataSet.clear()\n",
    "\n",
    "#------ For test images ------\n",
    "# faceMeshDataSet.extend(mainTestSet)\n",
    "\n",
    "# while len(faceMeshDataSet) > 0:\n",
    "#     croppedList = CropData(faceMeshDataSet, imagePerIter)\n",
    "#     label, landmarkVertices = LoadFaceMeshVertices(face_mesh, TESTIMAGEPATH, croppedList)\n",
    "\n",
    "#     count = 0\n",
    "#     for name, vertices in zip(croppedList, landmarkVertices):\n",
    "#         count += 1\n",
    "#         np.save(\"{}{}_landmark\".format(TESTLANDMARKPATH, name[1][:-4]), vertices)\n",
    "\n",
    "#     print(\"{} test landmark converted and saved\".format(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd71cd-9433-4c0a-9feb-fc38f79492b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing code for the loading of the conve\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True)\n",
    "\n",
    "faceMeshDataSet = []\n",
    "faceMeshDataSet.extend(mainTrainSet)\n",
    "croppedList = CropData(faceMeshDataSet, 80)\n",
    "\n",
    "# Data containing label, face image in rgb and normalizedlandmarklist\n",
    "label, landmarklists, faces = LoadFaceMeshes(face_mesh, TRAINIMAGEPATH, croppedList, True)\n",
    "DrawFaceLandmark(label[0], faces[0], landmarklists[0]) # Visualise the face landmark with face image here\n",
    "\n",
    "# Data containing label, data\n",
    "# label2, landmarkVertices = LoadFaceMeshVertices(face_mesh, TRAINIMAGEPATH, croppedList)\n",
    "\n",
    "# np.save(\"inputSample1\", landmarkVertices[0])\n",
    "# loadedData = np.load(\"inputSample1.npy\")\n",
    "# print(loadedData)\n",
    "# for label, face, mesh in zip(label, faces, data):\n",
    "#   DrawFaceLandmark(label,face,mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "#emotionList = ('Neutral 0', 'Happy 1', 'Sad 2', 'Surprise 3', 'Fear 4', 'Disgust 5', 'Anger 6', 'Contempt 7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb75463",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh_connections\n",
    "LandmarkPartsList = []\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_LIPS)\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_LEFT_EYE)\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_LEFT_IRIS)\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_LEFT_EYEBROW)\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_RIGHT_EYE)\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_RIGHT_EYEBROW)\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_RIGHT_IRIS)\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_FACE_OVAL)\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_NOSE)\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_CONTOURS)\n",
    "# LandmarkPartsList.append(mp_face_mesh.FACEMESH_IRISES)\n",
    "LandmarkPartsList.append(mp_face_mesh.FACEMESH_TESSELATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawCustomFaceLandmark(label[i], np.zeros((224,224,3), dtype=np.uint8), landmarklists[i], landmarkParts=LandmarkPartsList)\n",
    "i += 1\n",
    "# plt.imshow(faces[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(cv2.cvtColor(faces[i-1], cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a369ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(faces[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawFaceLandmark(label[i-1], faces[i-1], landmarklists[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848fb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "croppedList[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87ed7a-8592-4ab9-a33c-1a5d84de6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting single image, input correct image file name and path name\n",
    "vertices = MultiLandmarkToVertices(face_mesh.process(LoadImage(TRAINIMAGEPATH, \"10017.jpg\", normalize = False)).multi_face_landmarks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad63e438-d8b9-4e5f-932f-84485af0eece",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b50d61-7f37-4ccc-9385-b74bfa15d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=/training_information/logs/ --host=127.0.0.1 --port=8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d520d-29c4-42eb-9f4a-27049454ad2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pkg_resources\n",
    "# path = \"C:/Users/j.teoh/AppData/Local/miniconda3/envs/tflite-facial-expression/Lib/\"\n",
    "# pkg_resources.working_set.add_entry(path)\n",
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir training_information/logs/\n",
    "# %tensorboard --logdir training_information/logs/ --host localhost\n",
    "%tensorboard --inspect --logdir training_information/logs/  --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacd04d-92ef-4d15-b667-c3822b995226",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Accuracy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba274864-5875-4313-81c7-57728ea6cc04",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ModelPrediction(model, dataSet, imagesPerIter = 10000):\n",
    "  labelList = []\n",
    "  predictList = []\n",
    "  # test model\n",
    "  counter = 0\n",
    "  while(len(dataSet) != 0):\n",
    "    # training data\n",
    "    # try:\n",
    "      croppedList = CropData(dataSet, imagesPerIter)\n",
    "      counter += imagesPerIter\n",
    "      print(\"{} images loaded\".format(counter))\n",
    "      label, data = LoadVertices(TESTLANDMARKPATH, croppedList)\n",
    "      labelList.extend(label)\n",
    "      predictions = model.predict(data)\n",
    "      for pred in predictions:\n",
    "        result = np.argmax(pred)\n",
    "        predictList.append(result)\n",
    "    # except:\n",
    "      reset_keras()\n",
    "  \n",
    "  return labelList, predictList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f927f51-1f95-4c5c-9b3a-4881e5e2c6f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "modelFileName = \"ddfdd/ddfddFinal_35epoch_0.001lr_128bs_64_24_128_64_landmark.h5\"\n",
    "firstLayerSize = 64\n",
    "secondLayerSize = 24\n",
    "thirdLayerSize = 128\n",
    "fourthLayerSize = 64\n",
    "\n",
    "# Prepare model layers\n",
    "inputs = tf.keras.Input(shape=(478,3))\n",
    "base = tf.keras.layers.Dense(firstLayerSize, activation='relu')(inputs)\n",
    "base = tf.keras.layers.Dense(secondLayerSize, activation='relu')(base)\n",
    "base = tf.keras.layers.Flatten()(base)\n",
    "base = tf.keras.layers.Dense(thirdLayerSize, activation='relu')(base)\n",
    "base = tf.keras.layers.Dense(fourthLayerSize, activation='relu')(base)\n",
    "outputs = tf.keras.layers.Dense(8, activation='softmax')(base)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Load model\n",
    "model.load_weights(modelFileName)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# initialise image names and label\n",
    "dataset = mainTestSet.copy()\n",
    "\n",
    "# Test predict model\n",
    "labelList, predictList = ModelPrediction(model, dataset)\n",
    "\n",
    "# print accuracy score\n",
    "accuracy = accuracy_score(labelList, predictList)\n",
    "print(\"Neural Network accuracy: {}\".format(accuracy))\n",
    "\n",
    "# print confusion matrix\n",
    "cm = confusion_matrix(labelList, predictList)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(cm, annot = True, cmap = 'Blues', fmt = 'g', xticklabels = emotionList, yticklabels = emotionList)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Neural Network Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
