{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a214feb",
   "metadata": {},
   "source": [
    "## Import libraries and set up functions\n",
    "Starting directory is this \"..\\\\asset\" from this notebook code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75ea4e-7044-4150-b59e-d3a38ad33473",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from keras.backend import clear_session\n",
    "from keras.backend import get_session\n",
    "import time\n",
    "import math\n",
    "from datetime import timedelta\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"..\\\\asset\") # change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0796a-7779-40f1-a077-c013999ee3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotionList = ('Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt')\n",
    "TRAINANNOTATIONPATH = \"train_set\\\\annotations\\\\\"\n",
    "TRAINIMAGEPATH = \"train_set\\\\images\\\\\"\n",
    "TRAINLANDMARKPATH = \"train_set\\\\landmarks\\\\\"\n",
    "TESTANNOTATIONPATH = \"val_set\\\\annotations\\\\\"\n",
    "TESTIMAGEPATH = \"val_set\\\\images\\\\\"\n",
    "TESTLANDMARKPATH = \"val_set\\\\landmarks\\\\\"\n",
    "\n",
    "def LoadAllImageNames(filePath, limit = 0, catLimit = []):\n",
    "  \"\"\"Load the images names and label in tuple format (label, image name)\n",
    "\n",
    "  Args:\n",
    "      filePath (str): directory of image folder\n",
    "      limit (int, optional): max number of image to load. Defaults to 0.\n",
    "      catLimit (list<int>, optional): array of image count limit for each class. Defaults to []*8.\n",
    "\n",
    "  Returns:\n",
    "      list<str>: shuffled list of image names\n",
    "  \"\"\"\n",
    "  limitCounter = [0,0,0,0,0,0,0,0]\n",
    "  dataSet = []\n",
    "  loadCounter = 0\n",
    "  for file in os.scandir(filePath):\n",
    "    if (limit > 0 and loadCounter > limit):\n",
    "      break\n",
    "  \n",
    "    name = file.name[:-4] # file name w/o file extension\n",
    "    data = np.load(\"{}{}_exp.npy\".format(TRAINANNOTATIONPATH if filePath == TRAINIMAGEPATH else TESTANNOTATIONPATH, name)) # \n",
    "    label = int(data.item(0))\n",
    "\n",
    "    if len(catLimit) > 0 and limitCounter[label] >= catLimit[label]:\n",
    "        continue\n",
    "    limitCounter[label] += 1\n",
    "    loadCounter += 1\n",
    "\n",
    "    dataSet.append((label, file.name))\n",
    "    if (loadCounter%10000==0):\n",
    "      print(\"Files loaded:{}\".format(loadCounter))\n",
    "  \n",
    "  print(\"Total images loaded: \", loadCounter)\n",
    "  print(\"Images Loaded: \", limitCounter)\n",
    "  random.shuffle(dataSet)\n",
    "  return dataSet\n",
    "\n",
    "# Load the pixels of a picture to numpy.ndarray format. false for test set, true for training set\n",
    "# Return image in RGB format\n",
    "def LoadImage(imagePath, imageName, normalize = True):\n",
    "  \"\"\"Load image using numpy\n",
    "\n",
    "  Args:\n",
    "      imagePath (str): image path \n",
    "      imageName (str): image name\n",
    "      normalize (bool, optional): To normalize image or not. Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "      numpy array: x,y,3 array\n",
    "  \"\"\"\n",
    "  # print(\"{}{}{}\".format(os.getcwd(), \"\\\\\"+imagePath, imageName))\n",
    "  image_array = cv2.imread(\"{}{}{}\".format(os.getcwd(), \"\\\\\"+imagePath, imageName))\n",
    "  image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
    "  if normalize:\n",
    "    image_array = image_array/255\n",
    "    \n",
    "  return image_array\n",
    "\n",
    "# Extract the daata from 0 to amount from list and return it\n",
    "def CropData(list, amount):\n",
    "  if (len(list) < amount):\n",
    "    amount = len(list)\n",
    "  croppedList = list[:amount]\n",
    "  del list[:amount]\n",
    "  return croppedList\n",
    "\n",
    "def LoadImages(imagePath, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entries in list:\n",
    "    try:\n",
    "      image = LoadImage(imagePath, entries[1])\n",
    "      data.append(image) \n",
    "      label.append(entries[0]) \n",
    "    except: \n",
    "      print(\"Failed to load training image: \", entries[1])\n",
    "  npLabel = np.array(label) \n",
    "  npData = np.array(data) \n",
    "  return npLabel, npData \n",
    "\n",
    "# Load a list of face mesh in landmark_pb2.NormalizedLandmarkList format\n",
    "def LoadFaceMeshes(faceMesh, path, list, withFace = False):\n",
    "  face = []\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    # try:\n",
    "    if withFace:\n",
    "      face.append(LoadImage(path, entry[1], normalize = False)) # Load image\n",
    "      data.append(faceMesh.process(face[-1])) # process it into face mesh and append into data\n",
    "    else:\n",
    "      data.append(faceMesh.process(LoadImage(path, entry[1], normalize = False))) # Load image, process it into face mesh, and append into data\n",
    "    label.append(entry[0])\n",
    "    # except:\n",
    "      # print(\"Failed to load training image: \", entry[1])\n",
    "\n",
    "  if withFace:\n",
    "    return np.array(label), np.array(data), np.array(face)\n",
    "  else:\n",
    "    return np.array(label), np.array(data)\n",
    "\n",
    "# Load face mesh data in numpy array of \n",
    "def LoadFaceMeshVertices(faceMesh, path, list):\n",
    "  label = []\n",
    "  data = []\n",
    "  for entry in list:\n",
    "    # load image, process it into normalizedlandmarklist format, obtain face data from the list, convert it into vertices\n",
    "    data.append(MultiLandmarkToVertices(faceMesh.process(LoadImage(path, entry[1], normalize = False)).multi_face_landmarks[0]))\n",
    "    label.append(entry[0])\n",
    "    # except:\n",
    "      # print(\"Failed to load training image: \", entry[1])\n",
    "\n",
    "  return np.array(label), np.array(data)\n",
    "  \n",
    "def MultiLandmarkToVertices(multiLandmark):\n",
    "  verticesList = []\n",
    "  for idx, landmark in enumerate(multiLandmark.landmark):\n",
    "    verticesList.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "  return verticesList\n",
    "\n",
    "\n",
    "def InitializeModel():\n",
    "  pretrained_model = tf.keras.applications.MobileNetV3Large(input_shape=(224,224,3)) # Initializing model with mobile net V3 pretrained model\n",
    "\n",
    "  # Initializing the input and output from the model, removing last layer\n",
    "  base_input = pretrained_model.layers[0].input\n",
    "  base_output = pretrained_model.layers[-2].output\n",
    "\n",
    "  # Adding 3 more layers to output side\n",
    "  final_output = layers.Dense(128)(base_output) # Adding new layers, to the output side\n",
    "  final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "  final_output = layers.Dense(64)(final_output)\n",
    "  final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "  final_output = layers.Dense(8, activation = 'softmax')(final_output) # 8 cuz there are 8 image classifications\n",
    "\n",
    "  new_model = keras.Model(inputs = base_input, outputs = final_output)\n",
    "  # new_model.summary()\n",
    "  return new_model\n",
    "\n",
    "def ConvertToGray(image):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "  return image\n",
    "\n",
    "def ScaleImage(image, width):\n",
    "  ratio = image.shape[1]/width\n",
    "  image = cv2.resize(image, (width, int(image.shape[0]/ratio)))\n",
    "  return image\n",
    "\n",
    "def DetectFace(image):\n",
    "  face_roi = np.ndarray(1)\n",
    "  faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "  grayImage = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  faces = faceCascade.detectMultiScale(grayImage, 1.3, 5)\n",
    "  for x,y,w,h in faces:\n",
    "    roi_gray = grayImage[y:y+h, x:x+w]\n",
    "    roi_color = image[y:y+h, x:x+w]\n",
    "    cv2.rectangle(image, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "    facess = faceCascade.detectMultiScale(roi_gray)\n",
    "    if (len(facess) == 0):\n",
    "      print(\"Face not detected\")\n",
    "    else:\n",
    "      for (ex,ey,ew,eh) in facess:\n",
    "        face_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "  return face_roi\n",
    "\n",
    "def ConvertToInput(image):\n",
    "  input = ScaleImage(image, 224)\n",
    "  input = np.expand_dims(input, axis = 0) ## to add fourth dimension to fit model input\n",
    "  input = input/255\n",
    "  return input\n",
    "\n",
    "def GetResult(model, input):\n",
    "  Predictions = model.predict(input)\n",
    "  print(Predictions)\n",
    "  result = np.argmax(Predictions)\n",
    "  return emotionList[result]\n",
    "\n",
    "def printDataSetLabels(dataSet):\n",
    "    counterList = list(range(8))\n",
    "    for label, name in dataSet:\n",
    "        counterList[label] += 1\n",
    "    print(counterList)\n",
    "    \n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    # try:\n",
    "    #     del classifier # this is from global space - change this as you need\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.9, visible_device_list=\"0\")\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.times = []\n",
    "\n",
    "  def on_epoch_begin(self, batch, logs={}):\n",
    "    self.epoch_time_start = time.time()\n",
    "\n",
    "  def on_epoch_end(self, batch, logs={}):\n",
    "    self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "  def AverageTime(self):\n",
    "    sum = 0\n",
    "    for time in self.times:\n",
    "      sum += time\n",
    "    return sum/len(self.times)\n",
    "\n",
    "def DrawFaceLandmark(label, image, landmarks):\n",
    "  mp_drawing = mp.solutions.drawing_utils\n",
    "  mp_drawing_styles = mp.solutions.drawing_styles\n",
    "  mp_face_mesh = mp.solutions.face_mesh\n",
    "  print(f'Face landmarks of {label}:')\n",
    "  if not landmarks.multi_face_landmarks:\n",
    "    print(\"unagle to locate face landmark for {}\".format(label))\n",
    "  annotated_image = image.copy()\n",
    "  for face_landmarks in landmarks.multi_face_landmarks:\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_iris_connections_style())\n",
    "  plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d08f5",
   "metadata": {},
   "source": [
    "## Setup directory and import image file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8075535-75ed-424a-b01b-2689f93c99b7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get currect directory (os.getcwd() -> C:\\Users\\jazzt\\src)\n",
    "\n",
    "#-----------------------Start of code---------------------------\n",
    "# initialise image names and label\n",
    "mainTrainSet = LoadAllImageNames(TRAINIMAGEPATH, catLimit=[100]*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0bdc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Compiling and training model\n",
    "Using pretrained mode from MobileNetV3, fine-tuning of model can be done in the first block.\n",
    "In the second block, training of model can be done while tweaking the training parameters such as batch size and epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d25910",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.MobileNetV3Large(input_shape=(224,224,3)) # Initializing model with mobile net V3 pretrained model\n",
    "\n",
    "# Initializing the input and output from the model, removing last layer\n",
    "base_input = pretrained_model.layers[0].input\n",
    "base_output = pretrained_model.layers[-2].output\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Customizing layers\n",
    "#-------------------------------------------------\n",
    "# Adding 3 more layers to output side\n",
    "final_output = layers.Dense(128)(base_output) # Adding new layers, to the output side\n",
    "final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "final_output = layers.Dense(8, activation = 'softmax')(final_output) # 8 cuz there are 8 image classifications\n",
    "\n",
    "model = keras.Model(inputs = base_input, outputs = final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "time_callback = TimeHistory()\n",
    "dataSet = mainTrainSet.copy()\n",
    "\n",
    "# Initializing model fit params\n",
    "batchSize = 4\n",
    "imgPerIter = batchSize*16\n",
    "\n",
    "count = 0\n",
    "while(len(dataSet) != 0):\n",
    "  croppedList = []\n",
    "  try:\n",
    "    print(\"loading image\") # Crop and load images in\n",
    "    croppedList = CropData(dataSet, imgPerIter)\n",
    "    label, data = LoadImages(TRAINIMAGEPATH, croppedList)\n",
    "    \n",
    "    # Training model\n",
    "    model.fit(data, label, epochs = 11, batch_size = batchSize, callbacks = [time_callback])\n",
    "    print(time_callback.AverageTime())\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "  \n",
    "  # Saving weights\n",
    "  model.save_weights('bs48_128Dense.h5')\n",
    "  \n",
    "  # Print summary of current iteration\n",
    "  count = count + len(croppedList)\n",
    "  timeLeft = len(dataSet)/imgPerIter * time_callback.AverageTime()\n",
    "  print(\"trained image count: \", count)\n",
    "  print(\"Images Left: \", len(dataSet))\n",
    "  print(\"Estimated completion time: \", str(timedelta(seconds=timeLeft)))\n",
    "  \n",
    "  reset_keras()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f04772",
   "metadata": {},
   "source": [
    "## Testing a random image with a face\n",
    "The following block of code loads a image, crop the image to just the face, and convert it into model input before inserting it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc8dd0-074a-42dd-ae64-4e395ab45f7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing trained model\n",
    "image = LoadImage(TESTIMAGEPATH,\"3.jpg\", normalize = False)\n",
    "# image = cv2.resize(image, (224,224))\n",
    "# image = ScaleImage(image, 224)\n",
    "\n",
    "face = DetectFace(image)\n",
    "\n",
    "plt.imshow(face)\n",
    "\n",
    "preppedInput = ConvertToInput(face)\n",
    "result = GetResult(model, preppedInput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a660c-2224-497d-a4b5-e06cbf4ae544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing loading in data from tar using tensorflow\n",
    "# os.chdir(\"C:\\\\Users\\\\j.teoh\\\\Desktop\\\\tflite-facial-expression\") # change working directory\n",
    "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0dff4",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "The following block of code takes in a bunch of weight names and test their accuracy and lose before printing the result out.\n",
    "Insert the weight file names into \"modelNames\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c5782-12fa-4d90-9fac-f6a3ff6f1d4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Testing model\n",
    "\n",
    "modelNames = [\"Limit30000Label126BatchSize48gpu.h5\"]\n",
    "model = InitializeModel()\n",
    "mainData = LoadAllImageNames(TESTIMAGEPATH)\n",
    "resultPool =[]\n",
    "\n",
    "for modelName in modelNames:\n",
    "    model.load_weights(modelName)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # initialise image names and label\n",
    "    testSetData = []\n",
    "    testSetData.extend(mainData)\n",
    "    # print(mainData)\n",
    "    # test model\n",
    "    lostSum = 0\n",
    "    accuracySum = 0\n",
    "    count = 0\n",
    "    while(len(testSetData) != 0):\n",
    "      # training data\n",
    "      try:\n",
    "        croppedList = CropData(testSetData, 100)\n",
    "        print(\"loading image\")\n",
    "        label, data = LoadImages(TESTIMAGEPATH, croppedList)\n",
    "        result = model.evaluate(data, label, batch_size = 1)\n",
    "        lostSum += result[0]\n",
    "        accuracySum += result[1]\n",
    "      except:\n",
    "        print(\"Failed to train data\")\n",
    "\n",
    "      count += 1\n",
    "      reset_keras()\n",
    "      # print(count)\n",
    "    # print(mainData)\n",
    "    print(\"==========FINISH TESTING===========\")\n",
    "    print(\"model name: \", modelName)\n",
    "    print(\"average lost: \", lostSum/count)\n",
    "    print(\"average accuracy: \", accuracySum/count)\n",
    "    resultPool.append((modelName, lostSum/count, accuracySum/count))\n",
    "    \n",
    "print(resultPool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d07458",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting up image with Keras for MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe1de8-8627-440c-9cab-7626a70eb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "# preparing dataset of 1000 images\n",
    "dataset = trainingSetData[:1000].copy()\n",
    "trainingSet = []\n",
    "for label, fileName in dataset:\n",
    "    # Load image and convert to 224,224,3 nested array\n",
    "    tempImg = image.load_img(\"train_set\\\\images\\\\\"+fileName, target_size = (224,224))\n",
    "    tempImg = image.img_to_array(tempImg)\n",
    "    \n",
    "    # turn all the values in nested array into value between -1 and 1\n",
    "    trainingSet.append(preprocess_input(tempImg))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84796467",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GPU Setting\n",
    "Uncomment the first line of code to force the kernel to compute with CPU instead of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    # tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # config = tf.compat.v1.ConfigProto()\n",
    "    # config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "    # keras.set_session(tf.Session(config=config))\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bd416-877c-403e-8fa2-f3356d68de0c",
   "metadata": {},
   "source": [
    "## Mediapipe Testings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfffde2-81f1-493b-89c0-19eeb17a3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b9de0-069d-4134-8485-906a9ad3e4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(mp_face_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e74992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code load a list of face data in facemesh vertices and save it\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True)\n",
    "faceMeshDataSet = []\n",
    "faceMeshDataSet.extend(mainTrainSet)\n",
    "\n",
    "print(\"croppedList: \", len(croppedList))\n",
    "# Data containing label, data\n",
    "label, landmarkVertices = LoadFaceMeshVertices(face_mesh, TRAINIMAGEPATH, faceMeshDataSet)\n",
    "print(\"label: \", len(label))\n",
    "print(\"landmarkVertices: \", len(landmarkVertices))\n",
    "\n",
    "count = 0\n",
    "for name, vertices in zip(croppedList, landmarkVertices):\n",
    "    count += 1\n",
    "    np.save(\"{}{}_landmark\".format(TRAINLANDMARKPATH, name[1][:-4]), vertices)\n",
    "    # loadedData = np.load(\"inputSample1.npy\")\n",
    "    # print(loadedData)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd71cd-9433-4c0a-9feb-fc38f79492b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load drawing_utils and drawing_styles\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True)\n",
    "\n",
    "faceMeshDataSet = []\n",
    "faceMeshDataSet.extend(mainTrainSet)\n",
    "croppedList = CropData(faceMeshDataSet, 15)\n",
    "\n",
    "# Data containing label, face image in rgb and normalizedlandmarklist\n",
    "label, landmarklists, faces = LoadFaceMeshes(face_mesh, TRAINIMAGEPATH, croppedList, True)\n",
    "DrawFaceLandmark(label[0], faces[0], landmarklists[0])\n",
    "\n",
    "# Data containing label, data\n",
    "label2, landmarkVertices = LoadFaceMeshVertices(face_mesh, TRAINIMAGEPATH, croppedList)\n",
    "\n",
    "# print(landmarkVertices[0])\n",
    "\n",
    "\n",
    "# np.savetxt('inputSample.txt', np.array(data[0].multi_face_landmarks), delimiter=',')\n",
    "# def write_test5(vertices, file_path):\n",
    "# df = pd.DataFrame(data=np.array(landmarkVertices[0]))\n",
    "# df.insert(loc=0, column='v', value=\"v\")\n",
    "# df.to_csv('inputSample.csv', index=False, sep=\" \", header=False)\n",
    "\n",
    "np.save(\"inputSample1\", landmarkVertices[0])\n",
    "loadedData = np.load(\"inputSample1.npy\")\n",
    "print(loadedData)\n",
    "# for label, face, mesh in zip(label, faces, data):\n",
    "#   DrawFaceLandmark(label,face,mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = data[0].multi_face_landmarks[0]\n",
    "for idx, landmark in enumerate(landmarks.landmark):\n",
    "    print(type(idx), idx)\n",
    "    print(type(landmark), landmark)\n",
    "    print(type(landmark.x), landmark.x)\n",
    "    print(type(landmark.y), landmark.y)\n",
    "    print(type(landmark.z), landmark.z)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87ed7a-8592-4ab9-a33c-1a5d84de6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = LoadImage(TESTIMAGEPATH,\"3.jpg\", normalize = False)\n",
    "vertices = MultiLandmarkToVertices(face_mesh.process(LoadImage(TESTIMAGEPATH, \"3.jpg\", normalize = False)).multi_face_landmarks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dcada-a465-414d-bd50-3924f003a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MediaPipe Face Mesh.\n",
    "results = face_mesh.process(image)\n",
    "print(type(image))\n",
    "print(type(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a56b8-9ac6-4205-97ee-638619d8ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawFaceLandmark(\"3\", image, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09564c-d96d-47bd-9134-8d521f78224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceMeshDataSet = []\n",
    "faceMeshDataSet.extend(mainTrainSet)\n",
    "croppedList = CropData(, 100)\n",
    "print(\"loading image\")\n",
    "label, data = LoadImages(TESTIMAGEPATH, croppedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3702410-3250-4007-a7b8-5d6753d57f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
