{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a214feb",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a75ea4e-7044-4150-b59e-d3a38ad33473",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import multiprocessing\n",
    "from keras.backend import set_session\n",
    "from keras.backend import clear_session\n",
    "from keras.backend import get_session\n",
    "import time\n",
    "from datetime import timedelta \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a0796a-7779-40f1-a077-c013999ee3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotionList = ('Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt')\n",
    "TRAINANNOTATIONPATH = \"train_set\\\\annotations\\\\\"\n",
    "TRAINIMAGEPATH = \"train_set\\\\images\\\\\"\n",
    "TESTANNOTATIONPATH = \"val_set\\\\annotations\\\\\"\n",
    "TESTIMAGEPATH = \"val_set\\\\images\\\\\"\n",
    "\n",
    "def Setup():\n",
    "  os.chdir(\"C:\\\\Users\\\\j.teoh\\\\Desktop\\\\tflite-facial-expression\") # change working directory\n",
    "\n",
    "def LoadAllImageNames(filePath, imageFiles, limit = 0, catLimit = []*8):\n",
    "  \"\"\"Load the images names and label in tuple format (label, image name)\n",
    "\n",
    "  Args:\n",
    "      filePath (str): directory of folder the file is located\n",
    "      imageFiles (str): a list of image file names\n",
    "      limit (int, optional): max number of image to load. Defaults to 0.\n",
    "      catLimit (list<int>, optional): array of image count limit for each class. Defaults to []*8.\n",
    "\n",
    "  Returns:\n",
    "      list<str>: shuffled list of image names\n",
    "  \"\"\"\n",
    "  limitCounter = [0,0,0,0,0,0,0,0]\n",
    "  dataSet = []\n",
    "  loadCounter = 0\n",
    "  for file in imageFiles:\n",
    "    if (limit > 0 and loadCounter > limit):\n",
    "      break\n",
    "  \n",
    "    name = file.name[:-4] # file name w/o file extension\n",
    "    data = np.load(\"{}{}_exp.npy\".format(filePath, name)) # \n",
    "    label = int(data.item(0))\n",
    "\n",
    "    if limitCounter[label] >= catLimit[label]:\n",
    "      continue\n",
    "    limitCounter[label] += 1\n",
    "    loadCounter += 1\n",
    "\n",
    "    dataSet.append((label, file.name))\n",
    "    if (loadCounter%10000==0):\n",
    "      print(\"Files loaded:{}\".format(loadCounter))\n",
    "  \n",
    "  print(\"Total images loaded: \", loadCounter)\n",
    "  print(\"Images Loaded: \", limitCounter)\n",
    "  random.shuffle(dataSet)\n",
    "  return dataSet\n",
    "\n",
    "# Load the pixels of a picture to numpy.ndarray format. false for test set, true for training set\n",
    "# Return image in RGB format\n",
    "def LoadImage(imagePath, imageName, normalize = True):\n",
    "  \"\"\"Load image using numpy\n",
    "\n",
    "  Args:\n",
    "      imagePath (str): image path \n",
    "      imageName (str): image name\n",
    "      normalize (bool, optional): To normalize image or not. Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "      numpy array: x,y,3 array\n",
    "  \"\"\"\n",
    "  print(\"{}{}{}\".format(os.getcwd(), \"\\\\\"+imagePath, imageName))\n",
    "  image_array = cv2.imread(\"{}{}{}\".format(os.getcwd(), \"\\\\\"+imagePath, imageName))\n",
    "  image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
    "  if normalize:\n",
    "    image_array = image_array/255\n",
    "    \n",
    "  return image_array\n",
    "\n",
    "# Extract the daata from 0 to amount from list and return it\n",
    "def CropData(list, amount):\n",
    "  if (len(list) < amount):\n",
    "    amount = len(list)\n",
    "  croppedList = list[:amount]\n",
    "  del list[:amount]\n",
    "  return croppedList\n",
    "\n",
    "def LoadImages(list, trainingSetBool):\n",
    "  label = []\n",
    "  data = []\n",
    "  count = 0\n",
    "  for entries in list:\n",
    "    try:\n",
    "      image = LoadImage(entries[1], trainingSetBool)\n",
    "      data.append(image) \n",
    "      label.append(entries[0]) \n",
    "    except: \n",
    "      print(\"Failed to load training image: \", entries[ 1])\n",
    "  npLabel = np.array(label) \n",
    "  npData = np.array(data) \n",
    "  return npLabel, npData \n",
    "\n",
    "def InitializeModel():\n",
    "  pretrained_model = tf.keras.applications.MobileNetV3Large(input_shape=(224,224,3)) # Initializing model with mobile net V3 pretrained model\n",
    "\n",
    "  # Initializing the input and output from the model, removing last layer\n",
    "  base_input = pretrained_model.layers[0].input\n",
    "  base_output = pretrained_model.layers[-2].output\n",
    "\n",
    "  # Adding 3 more layers to output side\n",
    "  final_output = layers.Dense(128)(base_output) # Adding new layers, to the output side\n",
    "  final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "  final_output = layers.Dense(64)(final_output)\n",
    "  final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "  final_output = layers.Dense(8, activation = 'softmax')(final_output) # 8 cuz there are 8 image classifications\n",
    "\n",
    "  new_model = keras.Model(inputs = base_input, outputs = final_output)\n",
    "  # new_model.summary()\n",
    "  return new_model\n",
    "\n",
    "def ConvertToGray(image):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "  return image\n",
    "\n",
    "def ScaleImage(image, width):\n",
    "  ratio = image.shape[1]/width\n",
    "  image = cv2.resize(image, (width, int(image.shape[0]/ratio)))\n",
    "  return image\n",
    "\n",
    "def DetectFace(image):\n",
    "  face_roi = np.ndarray(1)\n",
    "  faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "  grayImage = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "  faces = faceCascade.detectMultiScale(grayImage, 1.3, 5)\n",
    "  for x,y,w,h in faces:\n",
    "    roi_gray = grayImage[y:y+h, x:x+w]\n",
    "    roi_color = image[y:y+h, x:x+w]\n",
    "    cv2.rectangle(image, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "    facess = faceCascade.detectMultiScale(roi_gray)\n",
    "    if (len(facess) == 0):\n",
    "      print(\"Face not detected\")\n",
    "    else:\n",
    "      for (ex,ey,ew,eh) in facess:\n",
    "        face_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "  return face_roi\n",
    "\n",
    "def ConvertToInput(image):\n",
    "  input = ScaleImage(image, 224)\n",
    "  input = np.expand_dims(input, axis = 0) ## to add fourth dimension to fit model input\n",
    "  input = input/255\n",
    "  return input\n",
    "\n",
    "def GetResult(model, input):\n",
    "  Predictions = model.predict(input)\n",
    "  print(Predictions)\n",
    "  result = np.argmax(Predictions)\n",
    "  return emotionList[result]\n",
    "\n",
    "def printDataSetLabels(dataSet):\n",
    "    counterList = list(range(8))\n",
    "    for label, name in dataSet:\n",
    "        counterList[label] += 1\n",
    "    print(counterList)\n",
    "    \n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    # try:\n",
    "    #     del classifier # this is from global space - change this as you need\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.9, visible_device_list=\"0\")\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.times = []\n",
    "\n",
    "  def on_epoch_begin(self, batch, logs={}):\n",
    "    self.epoch_time_start = time.time()\n",
    "\n",
    "  def on_epoch_end(self, batch, logs={}):\n",
    "    self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "  def AverageTime(self):\n",
    "    sum = 0\n",
    "    for time in self.times:\n",
    "      sum += time\n",
    "    return sum/len(self.times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d08f5",
   "metadata": {},
   "source": [
    "## Setup directory and import image file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8075535-75ed-424a-b01b-2689f93c99b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get currect directory (os.getcwd() -> C:\\Users\\jazzt\\src)\n",
    "\n",
    "#-----------------------Start of code---------------------------\n",
    "# Path directories\n",
    "Setup()\n",
    "\n",
    "# initialise image names and label\n",
    "imageFiles = os.scandir(\"train_set\\\\images\")\n",
    "mainTrainSet = LoadAllImageNames(TRAINANNOTATIONPATH, imageFiles, catLimit=[3000]*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0bdc1",
   "metadata": {},
   "source": [
    "## Compiling and training model\n",
    "Using pretrained mode from MobileNetV3, fine-tuning of model can be done in the first block.\n",
    "In the second block, training of model can be done while tweaking the training parameters such as batch size and epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d25910",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.MobileNetV3Large(input_shape=(224,224,3)) # Initializing model with mobile net V3 pretrained model\n",
    "\n",
    "# Initializing the input and output from the model, removing last layer\n",
    "base_input = pretrained_model.layers[0].input\n",
    "base_output = pretrained_model.layers[-2].output\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Customizing layers\n",
    "#-------------------------------------------------\n",
    "# Adding 3 more layers to output side\n",
    "final_output = layers.Dense(128)(base_output) # Adding new layers, to the output side\n",
    "final_output = layers.Activation('relu')(final_output) # activating layer\n",
    "final_output = layers.Dense(8, activation = 'softmax')(final_output) # 8 cuz there are 8 image classifications\n",
    "\n",
    "model = keras.Model(inputs = base_input, outputs = final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "time_callback = TimeHistory()\n",
    "dataSet = trainingSetData.copy()\n",
    "\n",
    "# Initializing model fit params\n",
    "batchSize = 48\n",
    "imgPerIter = batchSize*16\n",
    "\n",
    "count = 0\n",
    "while(len(dataSet) != 0):\n",
    "  croppedList = []\n",
    "  try:\n",
    "    print(\"loading image\") # Crop and load images in\n",
    "    croppedList = CropData(dataSet, imgPerIter)\n",
    "    label, data = LoadImages(croppedList, True)\n",
    "    \n",
    "    # Training model\n",
    "    model.fit(data, label, epochs = 11, batch_size = batchSize, callbacks = [time_callback])\n",
    "    print(time_callback.AverageTime())\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "  \n",
    "  # Saving weights\n",
    "  model.save_weights('bs48_128Dense.h5')\n",
    "  \n",
    "  # Print summary of current iteration\n",
    "  count = count + len(croppedList)\n",
    "  timeLeft = len(dataSet)/imgPerIter * time_callback.AverageTime()\n",
    "  print(\"trained image count: \", count)\n",
    "  print(\"Images Left: \", len(dataSet))\n",
    "  print(\"Estimated completion time: \", str(timedelta(seconds=timeLeft)))\n",
    "  \n",
    "  reset_keras()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f04772",
   "metadata": {},
   "source": [
    "## Testing a random image with a face\n",
    "The following block of code loads a image and format it to a correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26fc8dd0-074a-42dd-ae64-4e395ab45f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jazzt\\anaconda3\\envs\\tflite-facial-expression\\src\\tf-facial-expression\\val_set\\images\\3.jpg\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Testing trained model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mLoadImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTESTIMAGEPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# image = cv2.resize(image, (224,224))\u001b[39;00m\n\u001b[0;32m      4\u001b[0m image \u001b[38;5;241m=\u001b[39m ScaleImage(image, \u001b[38;5;241m224\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m, in \u001b[0;36mLoadImage\u001b[1;34m(imagePath, imageName, normalize)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mimagePath, imageName))\n\u001b[0;32m     61\u001b[0m image_array \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mimagePath, imageName))\n\u001b[1;32m---> 62\u001b[0m image_array \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m     64\u001b[0m   image_array \u001b[38;5;241m=\u001b[39m image_array\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# Testing trained model\n",
    "image = LoadImage(TESTIMAGEPATH,\"3.jpg\", normalize = False)\n",
    "# image = cv2.resize(image, (224,224))\n",
    "image = ScaleImage(image, 224)\n",
    "\n",
    "face = DetectFace(image)\n",
    "\n",
    "plt.imshow(face)\n",
    "\n",
    "preppedInput = ConvertToInput(face)\n",
    "result = GetResult(model, preppedInput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fce379-adfc-4815-804d-c9baaf1a39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a660c-2224-497d-a4b5-e06cbf4ae544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing loading in data from tar using tensorflow\n",
    "os.chdir(\"C:\\\\Users\\\\j.teoh\\\\Desktop\\\\tflite-facial-expression\") # change working directory\n",
    "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4648c-69c7-45df-8e95-37c1fec209e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, labels = next(img_gen.flow_from_directory(\"train_set.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eaf62b-2632-43fa-a535-3e56764e5394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "croppedList = CropData(trainingSetData, 1000)\n",
    "label, data = LoadImages(croppedList, True)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data,label)).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39fe4e-7f8d-46ab-bb09-37ab517c1731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show(image, label):\n",
    "  plt.figure()\n",
    "  plt.imshow(image)\n",
    "  plt.title(label)\n",
    "  plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb5f2b-4139-4439-a730-bc03139569ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = InitializeModel()\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1dd45-9738-4129-832c-6b5d981f2007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(dataset.repeat(), epochs=11, steps_per_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c5782-12fa-4d90-9fac-f6a3ff6f1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model\n",
    "Setup()\n",
    "\n",
    "modelNames = [\"label126Limited6BatchSize48gpu.h5\"]\n",
    "model = InitializeModel()\n",
    "mainData = LoadAllTestImageNames(os.scandir(\"val_set\\\\images\"))\n",
    "resultPool =[]\n",
    "\n",
    "for modelName in modelNames:\n",
    "    model.load_weights(modelName)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # initialise image names and label\n",
    "    testSetData = []\n",
    "    testSetData.extend(mainData)\n",
    "    # print(mainData)\n",
    "    # test model\n",
    "    lostSum = 0\n",
    "    accuracySum = 0\n",
    "    count = 0\n",
    "    while(len(testSetData) != 0):\n",
    "      # training data\n",
    "      # try:\n",
    "      croppedList = CropData(testSetData, 100)\n",
    "      print(\"loading image\")\n",
    "      label, data = LoadImages(croppedList, False)\n",
    "      result = model.evaluate(data, label, batch_size = 1)\n",
    "      lostSum += result[0]\n",
    "      accuracySum += result[1]\n",
    "      # except:\n",
    "      #   print(\"Failed to train data\")\n",
    "\n",
    "      count += 1\n",
    "      reset_keras()\n",
    "      # print(count)\n",
    "    # print(mainData)\n",
    "    print(\"==========FINISH TESTING===========\")\n",
    "    print(\"model name: \", modelName)\n",
    "    print(\"average lost: \", lostSum/count)\n",
    "    print(\"average accuracy: \", accuracySum/count)\n",
    "    resultPool.append((modelName, lostSum/count, accuracySum/count))\n",
    "    \n",
    "print(resultPool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d07458",
   "metadata": {},
   "source": [
    "## Setting up image with Keras for MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe1de8-8627-440c-9cab-7626a70eb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "Setup()\n",
    "# preparing dataset of 1000 images\n",
    "dataset = trainingSetData[:1000].copy()\n",
    "trainingSet = []\n",
    "for label, fileName in dataset:\n",
    "    # Load image and convert to 224,224,3 nested array\n",
    "    tempImg = image.load_img(\"train_set\\\\images\\\\\"+fileName, target_size = (224,224))\n",
    "    tempImg = image.img_to_array(tempImg)\n",
    "    \n",
    "    # turn all the values in nested array into value between -1 and 1\n",
    "    trainingSet.append(preprocess_input(tempImg))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84796467",
   "metadata": {},
   "source": [
    "## GPU Setting\n",
    "Uncomment the first line of code to force the kernel to compute with CPU instead of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    # tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # config = tf.compat.v1.ConfigProto()\n",
    "    # config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "    # keras.set_session(tf.Session(config=config))\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
